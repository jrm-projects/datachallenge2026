{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d75ee",
   "metadata": {},
   "source": [
    "# Axe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1beb3",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4437d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7153e13",
   "metadata": {},
   "source": [
    "### 1) Processing du df composant médical - Emission carbonne (kgCO2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees\n",
      "0    pansements composites                    0.9400              m2\n",
      "1           Sonde urinaire                   51.9034      parProduit\n",
      "2  Set de sondage urinaire                   89.4064      parProduit\n",
      "3      Collecteur de jambe                  250.7186      parProduit\n",
      "4       Collecteur de nuit                  676.6522      parProduit\n",
      "{'pansements composites': 2880.7506438973483, 'Sonde urinaire': 2.3058391190603773, 'Set de sondage urinaire': 1301.0278534529064, 'Collecteur de jambe': 1871.2095479103848, 'Collecteur de nuit': 2265.768898794339, 'Etuis péniens': 1088.7167868279912, 'Poche pour stomie': 3.131259971503482, 'Support pour stomie': 2959.0238083770532, 'Changes complets': 2375.889528275952, 'Slips absorbants': 250.72891706628525, 'Protections absorbantes': 1887.063988630245, 'Couches droites': 1329.456550312355, 'Alèses': 1383.9846297425656, 'pansement': 2495.056711895948, 'uteruscopes': 2579.2114069150794, 'instrument usage unique': 1233.5956502107485, 'bronchoscope': 2584.057077104993, 'verre lunettes': 570.6534648859387, 'Scanners': 1494.6674379613792, 'IRM (dont cage de Faraday. hélium)': 1479.9295216240887, 'Caméras à scintillation': 399.9716995350735, 'Tomographes à émission/ caméras à positons': 1949.3421425722856, 'Salles de radiologie': 2073.3922006142684, 'Appareils de mammographie': 568.0886777818828, 'Appareils de radiographie dentaire standard': 1571.6806541816616, 'Appareils de radiographie dentaire panoramique': 1792.2833323467103, 'Echographes portables': 2530.639912909539, 'Echographes fixes': 2426.4950561912756, 'Arceaux mobiles de radioscopie': 2592.063547762132, 'Statifs vasculaires avec arceau': 663.5845948900616, 'DAE': 1897.2515853423176, \"Générateurs d'hémodialyse\": 983.7099696401053, 'Equipements de radiothérapie': 1520.2436884291217, \"Ventilateurs d'anesthésie et de réanimation\": 197.9303483872975, 'Robots chirurgicaux': 1319.1239098347266, 'Pompes à perfusion': 596.6801935342754, 'Pousse-seringues': 1453.2358761107366, 'Pompes à nutrition (entérale ou parentérale)': 2705.902467790883, 'Autoclaves': 605.179419391841, 'Laveurs désinfecteurs': 2099.402660695002, 'Colonnes d’endoscopie et de coelioscopie': 1135.8599146287866, 'Moniteurs multi-paramétriques (ECG/PNI/SPO2. etc.)': 1912.330303059016, 'Aspirateurs de mucosités': 935.9956224870981, 'complement alimentaire': 1076.3137946700936}\n",
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees  \\\n",
      "0    pansements composites                    0.9400              m2   \n",
      "1           Sonde urinaire                   51.9034      parProduit   \n",
      "2  Set de sondage urinaire                   89.4064      parProduit   \n",
      "3      Collecteur de jambe                  250.7186      parProduit   \n",
      "4       Collecteur de nuit                  676.6522      parProduit   \n",
      "\n",
      "   Emission_carbonne_total_des_produits_kgCO2e  \n",
      "0                                 2.820000e+03  \n",
      "1                                 1.196809e+02  \n",
      "2                                 1.163202e+05  \n",
      "3                                 4.691470e+05  \n",
      "4                                 1.533138e+06  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def concat_xlsx_from_folder(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parcourt récursivement un dossier git et concatène verticalement\n",
    "    toutes les tables issues des fichiers .xlsx.\n",
    "\n",
    "    Hypothèses :\n",
    "    - chaque fichier .xlsx contient une table avec exactement 2 colonnes\n",
    "    - la première ligne correspond aux labels et est ignorée\n",
    "    - colonne A : produit\n",
    "    - colonne B : Emission_kgCO2e_unitaire\n",
    "    \"\"\"\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    all_rows = []\n",
    "\n",
    "    for file in folder.rglob(\"*.xlsx\"):\n",
    "        df = pd.read_excel(file, header=0)\n",
    "\n",
    "        df = df.iloc[:, :2]\n",
    "        df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "        filename = file.stem\n",
    "        if filename.endswith(\"_parProduit\"):\n",
    "            df[\"type_de_donnees\"] = \"parProduit\"\n",
    "        elif filename.endswith(\"_m2\"):\n",
    "            df[\"type_de_donnees\"] = \"m2\"\n",
    "        elif filename.endswith(\"_parKG\"):\n",
    "            df[\"type_de_donnees\"] = \"parKG\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_xlsx_from_folders(list_paths: list[str | Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatène verticalement les tables issues de plusieurs dossiers.\n",
    "\n",
    "    Paramètre\n",
    "    ----------\n",
    "    list_paths : list[str | Path]\n",
    "        Liste de chemins vers des dossiers contenant des fichiers .xlsx\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    DataFrame avec les colonnes :\n",
    "    - produit\n",
    "    - Emission_kgCO2e_unitaire\n",
    "    - type_de_donnees\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for folder_path in list_paths:\n",
    "        folder = Path(folder_path)\n",
    "\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Dossier introuvable : {folder}\")\n",
    "\n",
    "        xlsx_files = list(folder.rglob(\"*.xlsx\"))\n",
    "        if not xlsx_files:\n",
    "            raise ValueError(f\"Aucun fichier .xlsx trouvé dans {folder}\")\n",
    "\n",
    "        for file in xlsx_files:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "            name = file.stem\n",
    "            if name.endswith(\"_parProduit\"):\n",
    "                df[\"type_de_donnees\"] = \"parProduit\"\n",
    "            elif name.endswith(\"_m2\"):\n",
    "                df[\"type_de_donnees\"] = \"m2\"\n",
    "            elif name.endswith(\"_parKG\"):\n",
    "                df[\"type_de_donnees\"] = \"parKG\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def random_value_dict(\n",
    "    df: pd.DataFrame,\n",
    "    nom_col: str = \"produit\",\n",
    "    nom_to_ignore: list | None = None,\n",
    "    min_value: float = 1.0,\n",
    "    max_value: float = 3000.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Construit un dictionnaire :\n",
    "    - clés : valeurs uniques de df[nom_col]\n",
    "    - valeurs : nombre aléatoire strictement > 1\n",
    "      tiré dans [min_value, max_value]\n",
    "    \"\"\"\n",
    "\n",
    "    if nom_to_ignore is None:\n",
    "        nom_to_ignore = []\n",
    "\n",
    "    if min_value <= 1:\n",
    "        min_value = 1.000001\n",
    "\n",
    "    uniques = df[nom_col].dropna().unique()\n",
    "\n",
    "    return {\n",
    "        val: random.uniform(min_value, max_value)\n",
    "        for val in uniques\n",
    "        if val not in nom_to_ignore\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_emission_hopital(\n",
    "    df: pd.DataFrame,\n",
    "    type_de_donnees: str = \"type_de_donnees\",\n",
    "    dict_m2: dict = {\"pansements composites\": (3 * 10**2, 10)},\n",
    "    dict_parKG: dict = {\"instrument usage unique\": 1000, \"complement alimentaire\":10000},\n",
    "    dict_nb_parProduit: dict | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute la colonne Emission_carbonne_total_des_produits_kgCO2e selon\n",
    "    le type de données associé à chaque produit.\n",
    "    \"\"\"\n",
    "\n",
    "    if dict_nb_parProduit is None:\n",
    "        dict_nb_parProduit = {}\n",
    "\n",
    "    def compute_row(row):\n",
    "        produit = row[\"produit\"]\n",
    "        emission_unit = row[\"Emission_kgCO2e_unitaire\"]\n",
    "        t = row[type_de_donnees]\n",
    "\n",
    "        if t == \"parProduit\":\n",
    "            return emission_unit * dict_nb_parProduit.get(produit, np.nan)\n",
    "\n",
    "        if t == \"m2\":\n",
    "            longueur, largeur = dict_m2.get(produit, (np.nan, np.nan))\n",
    "            return emission_unit * longueur * largeur\n",
    "\n",
    "        if t == \"parKG\":\n",
    "            return emission_unit * dict_parKG.get(produit, np.nan)\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Emission_carbonne_total_des_produits_kgCO2e\"] = df.apply(compute_row, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 3. Construction du DataFrame\n",
    "# =========================\n",
    "\n",
    "# extract_path = \"sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# extract_path = \"C:/Users/jerem/Documents/GitHub/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "extract_path = \"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# paths = [\n",
    "#     r\"sujets\\chu\\Axe_2\\Axe_2_bdd-20260117T004817Z-1-001\\Axe_2_bdd\",\n",
    "#     r\"sujets\\chu\\Axe_2\\autre_dossier\"\n",
    "# ]\n",
    "\n",
    "df_concat = concat_xlsx_from_folder(extract_path)\n",
    "# df_concat = concat_xlsx_from_folders(paths)\n",
    "\n",
    "print(df_concat.head())\n",
    "\n",
    "# =========================\n",
    "# 4. Dictionnaires exemples\n",
    "# =========================\n",
    "\n",
    "dict_nb_parProduit = random_value_dict(df_concat)\n",
    "\n",
    "print(dict_nb_parProduit)\n",
    "\n",
    "# =========================\n",
    "# 5. Calcul des émissions\n",
    "# =========================\n",
    "\n",
    "df_final = compute_emission_hopital(\n",
    "    df_concat,\n",
    "    dict_nb_parProduit=dict_nb_parProduit\n",
    ")\n",
    "\n",
    "print(df_final.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7ee391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons sur la colonne 'produit' :\n",
      "Empty DataFrame\n",
      "Columns: [produit, Emission_kgCO2e_unitaire, type_de_donnees, Emission_carbonne_total_des_produits_kgCO2e]\n",
      "Index: []\n",
      "\n",
      "Valeurs répétées dans 'produit' :\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Doublons sur une variable particulière (ex: 'email')\n",
    "colonne = 'produit'\n",
    "\n",
    "# Trouver les valeurs dupliquées dans cette colonne\n",
    "valeurs_doublons = df_final[df_final.duplicated(subset=[colonne], keep=False)]\n",
    "\n",
    "# Afficher les doublons triés pour mieux voir\n",
    "doublons_tries = valeurs_doublons.sort_values(colonne)\n",
    "print(f\"Doublons sur la colonne '{colonne}' :\")\n",
    "print(doublons_tries)\n",
    "\n",
    "# Voir les valeurs qui se répètent\n",
    "comptage = df_final[colonne].value_counts()\n",
    "valeurs_repetees = comptage[comptage > 1]\n",
    "print(f\"\\nValeurs répétées dans '{colonne}' :\")\n",
    "print(valeurs_repetees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f786ffa",
   "metadata": {},
   "source": [
    "#### 1.1) Obtention du dataframe final et exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbd6cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_conserver = [\n",
    "    \"produit\",\n",
    "    \"Emission_kgCO2e_unitaire\",\n",
    "    \"Emission_carbonne_total_des_produits_kgCO2e\"\n",
    "]\n",
    "\n",
    "df_export = df_final[colonnes_a_conserver].copy()\n",
    "\n",
    "# =========================\n",
    "# Chemin de sortie\n",
    "# =========================\n",
    "\n",
    "output_path = Path(r\"results\\df_composant_medical_emissions_carbones.xlsx\")\n",
    "\n",
    "# Création du dossier si besoin\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Export Excel\n",
    "# =========================\n",
    "\n",
    "df_export.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a2ec7",
   "metadata": {},
   "source": [
    "### 2) Pre processing NLP des bases **df_composant_medical_emissions_carbones.xlsx** et **DISPOSITIFS_MED.xlsx** et Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d8d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sup un encoding: utf-8\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# !pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# !pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# !pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# !pip install spacy transformers sentencepiece torch\n",
    "# !python -m spacy download fr_core_news_md\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install rank-bm25\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "# !pip install openpyxl\n",
    "# !pip install sentencepiece\n",
    "\n",
    "# RAPPEL : relancer le kernel si pb détection de packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb80a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"CUDA available :\", torch.cuda.is_available())\n",
    "# print(\"GPU name :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# import torch\n",
    "# print(torch.cuda.memory_allocated() / 1e9, \"GB GPU used\")\n",
    "\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.75it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:04<00:00,  9.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    # device: int = -1  # -1 CPU, 0 GPU\n",
    "    device: int = 0  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \"\"\"\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # calcul similarities restreint\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        sims[i, :] = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy()}\n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # print(df1.head())\n",
    "    # print(df2.head())\n",
    "\n",
    "    # TEMPS : 1min9s\n",
    "    # NLP + traduction (1min9s)\n",
    "    translator = TranslatorFR2EN(device=0)  # GPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "\n",
    "    # TEMPS : 24s\n",
    "    # Matching BM25 -> Embeddings -> Top5 \n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "\n",
    "    # # Sauvegarde\n",
    "    # match_wide.to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5.xlsx\", index=False)\n",
    "    # match_wide.attrs[\"out_long\"].to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_long.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98166e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df1 anglais\n",
    "# print(df1p.head()) \n",
    "\n",
    "# # df2 anglais\n",
    "# print(df2p.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab021d65",
   "metadata": {},
   "source": [
    "### 2bis) Version où on remplace la similarité cosinus par une pénalisation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b61439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.62it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:04<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul TF-IDF...\n",
      "Calcul similarités hybrides (embeddings + TF-IDF)...\n",
      "Calcul métriques d'incertitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8686/3628384705.py:316: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = spearmanr(embedding_sims_all[i], tfidf_sims_all[i])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    # device: int = -1  # -1 CPU, 0 GPU\n",
    "    device: int = 0  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  DIFFÉRENCE ENTRE LES DEUX APPROCHES DE SIMILARITÉ                            ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# 1) SIMILARITÉ COSINUS PURE (commentée ci-dessous) :\n",
    "#    - Mesure uniquement l'orientation des vecteurs embeddings\n",
    "#    - Ignore complètement les mots exacts utilisés\n",
    "#    - Exemple : \"cathéter veineux\" vs \"tube sanguin\" → score élevé (sémantique)\n",
    "#    - Problème : peut matcher des produits sémantiquement proches mais techniquement \n",
    "#      différents (ex: \"seringue 5ml\" vs \"seringue 10ml\")\n",
    "\n",
    "# 2) SIMILARITÉ HYBRIDE COSINUS + TF-IDF (implémentation actuelle) :\n",
    "#    - Combine sémantique (embeddings) + lexical (TF-IDF)\n",
    "#    - TF-IDF donne plus de poids aux termes rares/spécifiques\n",
    "#    - Exemple : \"cathéter ventriculaire\" → \"ventriculaire\" pèse plus lourd que \n",
    "#      \"cathéter\" (plus commun)\n",
    "#    - Avantage : détecte les correspondances exactes de termes techniques tout en \n",
    "#      gardant la compréhension sémantique\n",
    "#    - Paramètre alpha : contrôle l'équilibre entre les deux mesures\n",
    "#      * alpha=1.0 → 100% embeddings (cosinus pur)\n",
    "#      * alpha=0.0 → 100% TF-IDF (lexical pur)\n",
    "#      * alpha=0.6 → compromis pour nomenclatures médicales\n",
    "\n",
    "# INTERPRÉTABILITÉ :\n",
    "# - Score final = (0.6 × similarité_sémantique) + (0.4 × importance_termes_communs)\n",
    "# - Favorise les produits qui sont à la fois :\n",
    "#   1) Sémantiquement proches (compris par le modèle)\n",
    "#   2) Partageant des termes techniques spécifiques\n",
    "\n",
    "\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    "    alpha: float = 0.6,  # Pondération embeddings vs TF-IDF\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings + TF-IDF rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \n",
    "    Paramètres:\n",
    "        alpha: Poids des embeddings (1-alpha = poids TF-IDF)\n",
    "               alpha=1.0 → similarité cosinus pure\n",
    "               alpha=0.6 → recommandé (équilibre sémantique/lexical)\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.stats import spearmanr\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # TF-IDF sur corpus df1 unique\n",
    "    print(\"Calcul TF-IDF...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_matrix_df1 = vectorizer.fit_transform(df1u[df1_text_col].fillna(\"\"))\n",
    "    \n",
    "    # TF-IDF sur df2\n",
    "    tfidf_matrix_df2 = vectorizer.transform(df2[df2_text_col].fillna(\"\"))\n",
    "\n",
    "    # calcul similarities restreint (HYBRIDE)\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "    \n",
    "    # Stockage pour calcul d'incertitude\n",
    "    embedding_sims_all = np.empty((n2, topk_bm25), dtype=float)\n",
    "    tfidf_sims_all = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    print(\"Calcul similarités hybrides (embeddings + TF-IDF)...\")\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        \n",
    "        # # APPROCHE 1 (commentée) : Similarité cosinus pure sur embeddings\n",
    "        # cos_sim = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "        # sims[i, :] = cos_sim\n",
    "        \n",
    "        # APPROCHE 2 (actuelle) : Hybride embeddings + TF-IDF\n",
    "        # 1) Similarité sémantique (embeddings)\n",
    "        embedding_sim = emb2[i] @ emb1[idx].T  # (topk_bm25,)\n",
    "        \n",
    "        # 2) Similarité lexicale (TF-IDF sur termes communs)\n",
    "        tfidf_sim = (tfidf_matrix_df2[i] @ tfidf_matrix_df1[idx].T).toarray()[0]  # (topk_bm25,)\n",
    "        \n",
    "        # Stockage pour métriques d'incertitude\n",
    "        embedding_sims_all[i, :] = embedding_sim\n",
    "        tfidf_sims_all[i, :] = tfidf_sim\n",
    "        \n",
    "        # 3) Combinaison pondérée\n",
    "        sims[i, :] = alpha * embedding_sim + (1 - alpha) * tfidf_sim\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "    \n",
    "    # ============================================================\n",
    "    # QUANTIFICATION D'INCERTITUDE\n",
    "    # ============================================================\n",
    "    \n",
    "    # 1) Entropie normalisée de la distribution de probabilités\n",
    "    # Interprétation: >0.7 = très incertain (probs uniformes)\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
    "    normalized_entropy = entropy / np.log(topk_bm25)\n",
    "    \n",
    "    # 2) Variance des similarités brutes (avant softmax)\n",
    "    # Interprétation: variance élevée = scores bien différenciés (bon signal)\n",
    "    #                 variance faible = tous les candidats se ressemblent (mauvais signal)\n",
    "    sim_variance = np.var(sims, axis=1)\n",
    "    \n",
    "    # 3) Désaccord entre embeddings et TF-IDF (corrélation de Spearman)\n",
    "    # Interprétation: >0.5 = sémantique et lexique pointent vers candidats différents\n",
    "    print(\"Calcul métriques d'incertitude...\")\n",
    "    disagreement = np.zeros(n2)\n",
    "    for i in range(n2):\n",
    "        corr, _ = spearmanr(embedding_sims_all[i], tfidf_sims_all[i])\n",
    "        disagreement[i] = 1 - corr  # 0=accord parfait, 1=désaccord total\n",
    "    \n",
    "    # ============================================================\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\n",
    "        \"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy(),\n",
    "        \"uncertainty_entropy\": normalized_entropy,\n",
    "        \"similarity_variance\": sim_variance,\n",
    "        \"embedding_tfidf_disagreement\": disagreement,\n",
    "    }\n",
    "    \n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # print(df1.head())\n",
    "    # print(df2.head())\n",
    "\n",
    "    # TEMPS : 1min8s\n",
    "    # NLP + traduction (1min9s)\n",
    "    translator = TranslatorFR2EN(device=0)  # GPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "\n",
    "    # TEMPS : 9.7s\n",
    "    # Matching BM25 -> Embeddings+TF-IDF -> Top5 \n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07,\n",
    "        alpha=0.70  # 70% similarité cos, 30% TF-IDF\n",
    "        # alpha=0  # 70% similarité cos, 30% TF-IDF\n",
    "    )\n",
    "\n",
    "    # # Sauvegarde\n",
    "    # match_wide.to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_simcos_et_TF_IDF.xlsx\", index=False)\n",
    "    # match_wide.attrs[\"out_long\"].to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_long_simcos_et_TF_IDF.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e87b6",
   "metadata": {},
   "source": [
    "### 3) Réduction et évaluation des coûts carbones \n",
    "\n",
    "Le site de [EcoLogits](https://ecologits.ai/latest/reference/tracers/utils/#tracers.utils.llm_impacts) peut être utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d582a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLM, TintBERT ?\n",
    "# Métriques et graphes de consommation carbonne par inférence\n",
    "\n",
    "\n",
    "# INTÉGRATION ECOLOGITS + CODECARBON\n",
    "# ===================================\n",
    "\n",
    "# Combine les deux outils pour une analyse complète :\n",
    "# - CodeCarbon : mesures générales (spaCy, TF-IDF, BM25)\n",
    "# - EcoLogits : métriques détaillées pour LLMs (traduction, embeddings)\n",
    "\n",
    "# Installation : pip install ecologits codecarbon\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "\n",
    "# !pip install codecarbon\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# !pip install ecologits\n",
    "from ecologits.tracers.utils import llm_impacts\n",
    "ECOLOGITS_AVAILABLE = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a198c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure spaCy français...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BaseEmissionsTracker.__init__() got an unexpected keyword argument 'country_2letter_iso_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 338\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# 1) spaCy (classique)\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📊 Mesure spaCy français...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbench\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrack_classic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspaCy_fr_core_news_md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mspacy\u001b[39;49;00m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnlp_fr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfr_core_news_md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36m_ClassicModelTracker.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mself\u001b[39m.start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracker = \u001b[43mEmissionsTracker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcountry_2letter_iso_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcountry_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWARNING\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     \u001b[38;5;28mself\u001b[39m.tracker.start()\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: BaseEmissionsTracker.__init__() got an unexpected keyword argument 'country_2letter_iso_code'"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class EnhancedModelMetrics:\n",
    "    \"\"\"Métriques enrichies combinant CodeCarbon + EcoLogits\"\"\"\n",
    "    model_name: str\n",
    "    model_type: str  # \"llm\" ou \"classic\"\n",
    "    \n",
    "    # Métriques communes\n",
    "    duration_s: float\n",
    "    energy_kwh: float\n",
    "    \n",
    "    # Métriques CodeCarbon (tous modèles)\n",
    "    co2_kg: float\n",
    "    cpu_power_w: float = 0.0\n",
    "    gpu_power_w: float = 0.0\n",
    "    ram_power_w: float = 0.0\n",
    "    \n",
    "    # Métriques EcoLogits (LLMs uniquement)\n",
    "    gwp_kg_co2eq: Optional[float] = None      # Global Warming Potential\n",
    "    adpe_kg_sb_eq: Optional[float] = None     # Abiotic Depletion (métaux)\n",
    "    pe_mj: Optional[float] = None             # Primary Energy\n",
    "    wcf_liters: Optional[float] = None        # Water Consumption\n",
    "    usage_gwp: Optional[float] = None         # GWP usage seulement\n",
    "    embodied_gwp: Optional[float] = None      # GWP embodied seulement\n",
    "    \n",
    "    # Métadonnées LLM\n",
    "    tokens_processed: Optional[int] = None\n",
    "    latency_per_token_ms: Optional[float] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        base = {\n",
    "            \"Modèle\": self.model_name,\n",
    "            \"Type\": self.model_type,\n",
    "            \"Durée (s)\": round(self.duration_s, 2),\n",
    "            \"Énergie (kWh)\": round(self.energy_kwh, 6),\n",
    "            \"CO2 CodeCarbon (kg)\": round(self.co2_kg, 6),\n",
    "        }\n",
    "        \n",
    "        # Ajout métriques EcoLogits si disponibles\n",
    "        if self.model_type == \"llm\" and self.gwp_kg_co2eq is not None:\n",
    "            base.update({\n",
    "                \"GWP total (kg CO2eq)\": round(self.gwp_kg_co2eq, 6),\n",
    "                \"GWP usage (kg)\": round(self.usage_gwp or 0, 6),\n",
    "                \"GWP embodied (kg)\": round(self.embodied_gwp or 0, 6),\n",
    "                \"ADPe (kg Sb eq)\": round(self.adpe_kg_sb_eq or 0, 9),\n",
    "                \"Énergie primaire (MJ)\": round(self.pe_mj or 0, 3),\n",
    "                \"Eau (litres)\": round(self.wcf_liters or 0, 3),\n",
    "                \"Tokens traités\": self.tokens_processed or 0,\n",
    "                \"Latence/token (ms)\": round(self.latency_per_token_ms or 0, 2),\n",
    "            })\n",
    "        \n",
    "        return base\n",
    "\n",
    "\n",
    "class HybridCarbonBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark hybride utilisant CodeCarbon ET EcoLogits\n",
    "    \n",
    "    Usage:\n",
    "        bench = HybridCarbonBenchmark()\n",
    "        \n",
    "        # Modèle classique (spaCy)\n",
    "        with bench.track_classic(\"spacy_fr\"):\n",
    "            nlp = spacy.load(\"fr_core_news_md\")\n",
    "            docs = list(nlp.pipe(texts))\n",
    "        \n",
    "        # Modèle LLM (traduction)\n",
    "        with bench.track_llm(\"translation\", provider=\"huggingface\", model=\"Helsinki-NLP/opus-mt-fr-en\"):\n",
    "            outputs = translator(texts)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, country_code: str = \"FRA\", project_name: str = \"medical_matching\"):\n",
    "        self.country_code = country_code\n",
    "        self.project_name = project_name\n",
    "        self.results: List[EnhancedModelMetrics] = []\n",
    "        \n",
    "        if not ECOLOGITS_AVAILABLE:\n",
    "            print(\"⚠️ EcoLogits non disponible - métriques limitées à CodeCarbon\")\n",
    "    \n",
    "    def track_classic(self, model_name: str):\n",
    "        \"\"\"Track un modèle classique (spaCy, sklearn, etc.) avec CodeCarbon\"\"\"\n",
    "        return _ClassicModelTracker(self, model_name)\n",
    "    \n",
    "    def track_llm(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        provider: str = \"huggingface\",\n",
    "        model_id: str = None,\n",
    "        electricity_mix_zone: str = None\n",
    "    ):\n",
    "        \"\"\"Track un LLM avec CodeCarbon + EcoLogits\"\"\"\n",
    "        return _LLMTracker(self, model_name, provider, model_id, electricity_mix_zone)\n",
    "    \n",
    "    def add_result(self, metrics: EnhancedModelMetrics):\n",
    "        self.results.append(metrics)\n",
    "    \n",
    "    def get_dataframe(self) -> pd.DataFrame:\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame([r.to_dict() for r in self.results])\n",
    "    \n",
    "    def print_summary(self):\n",
    "        if not self.results:\n",
    "            print(\"Aucune mesure disponible\")\n",
    "            return\n",
    "        \n",
    "        df = self.get_dataframe()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"📊 RAPPORT CARBONE HYBRIDE - Projet: {self.project_name}\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Séparation LLM vs Classic\n",
    "        llm_results = [r for r in self.results if r.model_type == \"llm\"]\n",
    "        classic_results = [r for r in self.results if r.model_type == \"classic\"]\n",
    "        \n",
    "        print(f\"\\n🔬 Modèles classiques: {len(classic_results)}\")\n",
    "        print(f\"🤖 Modèles LLM: {len(llm_results)}\")\n",
    "        \n",
    "        # Total CodeCarbon\n",
    "        total_co2_cc = sum(r.co2_kg for r in self.results)\n",
    "        total_energy = sum(r.energy_kwh for r in self.results)\n",
    "        \n",
    "        print(f\"\\n⚡ Énergie totale (CodeCarbon): {total_energy:.6f} kWh\")\n",
    "        print(f\"🏭 CO2 total (CodeCarbon): {total_co2_cc:.6f} kg\")\n",
    "        \n",
    "        # Si EcoLogits disponible, afficher métriques enrichies\n",
    "        if llm_results and llm_results[0].gwp_kg_co2eq is not None:\n",
    "            total_gwp = sum(r.gwp_kg_co2eq or 0 for r in llm_results)\n",
    "            total_adpe = sum(r.adpe_kg_sb_eq or 0 for r in llm_results)\n",
    "            total_pe = sum(r.pe_mj or 0 for r in llm_results)\n",
    "            total_water = sum(r.wcf_liters or 0 for r in llm_results)\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*100)\n",
    "            print(\"🌍 MÉTRIQUES ENRICHIES ECOLOGITS (LLMs uniquement)\")\n",
    "            print(\"-\"*100)\n",
    "            print(f\"• GWP total: {total_gwp:.6f} kg CO2eq\")\n",
    "            print(f\"  ├─ Usage: {sum(r.usage_gwp or 0 for r in llm_results):.6f} kg\")\n",
    "            print(f\"  └─ Embodied: {sum(r.embodied_gwp or 0 for r in llm_results):.6f} kg\")\n",
    "            print(f\"• ADPe (épuisement métaux): {total_adpe:.9f} kg Sb eq\")\n",
    "            print(f\"• Énergie primaire: {total_pe:.3f} MJ\")\n",
    "            print(f\"• Consommation d'eau: {total_water:.3f} litres\")\n",
    "            \n",
    "            # Équivalences eau\n",
    "            print(f\"\\n💧 Équivalent eau:\")\n",
    "            print(f\"  • {total_water / 0.25:.0f} verres d'eau (250ml)\")\n",
    "            print(f\"  • {total_water / 8:.1f} douches (8L/min pendant 1min)\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"Détail par modèle:\")\n",
    "        print(\"-\"*100)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Équivalences carbone\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"🌳 ÉQUIVALENCES CARBONE\")\n",
    "        print(\"=\"*100)\n",
    "        km_voiture = total_co2_cc / 0.12\n",
    "        arbres_an = total_co2_cc / 21\n",
    "        smartphones = total_energy * 1000 / 0.012\n",
    "        \n",
    "        print(f\"• {km_voiture:.1f} km en voiture\")\n",
    "        print(f\"• {arbres_an:.2f} arbres pendant 1 an pour compenser\")\n",
    "        print(f\"• {smartphones:.0f} charges de smartphone\")\n",
    "    \n",
    "    def save_results(self, filepath: str):\n",
    "        df = self.get_dataframe()\n",
    "        if df.empty:\n",
    "            print(\"Aucun résultat à sauvegarder\")\n",
    "            return\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Métriques complètes\", index=False)\n",
    "            \n",
    "            # Onglet comparaison LLM vs Classic\n",
    "            comparison = pd.DataFrame({\n",
    "                \"Type\": [\"Modèles classiques\", \"Modèles LLM\", \"TOTAL\"],\n",
    "                \"Nombre\": [\n",
    "                    len([r for r in self.results if r.model_type == \"classic\"]),\n",
    "                    len([r for r in self.results if r.model_type == \"llm\"]),\n",
    "                    len(self.results)\n",
    "                ],\n",
    "                \"CO2 (kg)\": [\n",
    "                    sum(r.co2_kg for r in self.results if r.model_type == \"classic\"),\n",
    "                    sum(r.co2_kg for r in self.results if r.model_type == \"llm\"),\n",
    "                    sum(r.co2_kg for r in self.results)\n",
    "                ],\n",
    "                \"Énergie (kWh)\": [\n",
    "                    sum(r.energy_kwh for r in self.results if r.model_type == \"classic\"),\n",
    "                    sum(r.energy_kwh for r in self.results if r.model_type == \"llm\"),\n",
    "                    sum(r.energy_kwh for r in self.results)\n",
    "                ]\n",
    "            })\n",
    "            comparison.to_excel(writer, sheet_name=\"LLM vs Classic\", index=False)\n",
    "        \n",
    "        print(f\"✅ Résultats sauvegardés : {filepath}\")\n",
    "\n",
    "\n",
    "class _ClassicModelTracker:\n",
    "    \"\"\"Tracker pour modèles classiques (CodeCarbon uniquement)\"\"\"\n",
    "    \n",
    "    def __init__(self, benchmark: HybridCarbonBenchmark, model_name: str):\n",
    "        self.benchmark = benchmark\n",
    "        self.model_name = model_name\n",
    "        self.tracker = None\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.tracker = EmissionsTracker(\n",
    "            project_name=self.benchmark.project_name,\n",
    "            country_2letter_iso_code=self.benchmark.country_code,\n",
    "            log_level=\"WARNING\",\n",
    "        )\n",
    "        self.tracker.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        emissions = self.tracker.stop()\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        metrics = EnhancedModelMetrics(\n",
    "            model_name=self.model_name,\n",
    "            model_type=\"classic\",\n",
    "            duration_s=duration,\n",
    "            energy_kwh=self.tracker._total_energy.kWh if hasattr(self.tracker, '_total_energy') else 0,\n",
    "            co2_kg=emissions if emissions else 0,\n",
    "            cpu_power_w=self.tracker._cpu_power.W if hasattr(self.tracker, '_cpu_power') else 0,\n",
    "            gpu_power_w=self.tracker._gpu_power.W if hasattr(self.tracker, '_gpu_power') else 0,\n",
    "            ram_power_w=self.tracker._ram_power.W if hasattr(self.tracker, '_ram_power') else 0,\n",
    "        )\n",
    "        \n",
    "        self.benchmark.add_result(metrics)\n",
    "\n",
    "\n",
    "class _LLMTracker:\n",
    "    \"\"\"Tracker pour LLMs (CodeCarbon + EcoLogits)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        benchmark: HybridCarbonBenchmark, \n",
    "        model_name: str,\n",
    "        provider: str,\n",
    "        model_id: str,\n",
    "        electricity_mix_zone: str\n",
    "    ):\n",
    "        self.benchmark = benchmark\n",
    "        self.model_name = model_name\n",
    "        self.provider = provider\n",
    "        self.model_id = model_id or model_name\n",
    "        self.electricity_mix_zone = electricity_mix_zone or benchmark.country_code\n",
    "        self.tracker = None\n",
    "        self.start_time = None\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.tracker = EmissionsTracker(\n",
    "            project_name=self.benchmark.project_name,\n",
    "            country_2letter_iso_code=self.benchmark.country_code,\n",
    "            log_level=\"WARNING\",\n",
    "        )\n",
    "        self.tracker.start()\n",
    "        return self\n",
    "    \n",
    "    def set_token_count(self, count: int):\n",
    "        \"\"\"Permet de définir le nombre de tokens traités\"\"\"\n",
    "        self.token_count = count\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        emissions_cc = self.tracker.stop()\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        # Métriques CodeCarbon\n",
    "        energy_kwh = self.tracker._total_energy.kWh if hasattr(self.tracker, '_total_energy') else 0\n",
    "        \n",
    "        # Métriques EcoLogits (si disponible)\n",
    "        gwp = adpe = pe = wcf = usage_gwp = embodied_gwp = None\n",
    "        latency_per_token = None\n",
    "        \n",
    "        if ECOLOGITS_AVAILABLE and self.token_count > 0:\n",
    "            try:\n",
    "                impacts = llm_impacts(\n",
    "                    provider=self.provider,\n",
    "                    model_name=self.model_id,\n",
    "                    output_token_count=self.token_count,\n",
    "                    request_latency=duration,\n",
    "                    electricity_mix_zone=self.electricity_mix_zone,\n",
    "                )\n",
    "                \n",
    "                if impacts.gwp:\n",
    "                    gwp = impacts.gwp.value\n",
    "                    usage_gwp = impacts.usage.gwp.value if impacts.usage and impacts.usage.gwp else None\n",
    "                    embodied_gwp = impacts.embodied.gwp.value if impacts.embodied and impacts.embodied.gwp else None\n",
    "                \n",
    "                adpe = impacts.adpe.value if impacts.adpe else None\n",
    "                pe = impacts.pe.value if impacts.pe else None\n",
    "                wcf = impacts.wcf.value if impacts.wcf else None\n",
    "                latency_per_token = (duration / self.token_count) * 1000  # ms\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur EcoLogits pour {self.model_name}: {e}\")\n",
    "        \n",
    "        metrics = EnhancedModelMetrics(\n",
    "            model_name=self.model_name,\n",
    "            model_type=\"llm\",\n",
    "            duration_s=duration,\n",
    "            energy_kwh=energy_kwh,\n",
    "            co2_kg=emissions_cc if emissions_cc else 0,\n",
    "            gwp_kg_co2eq=gwp,\n",
    "            adpe_kg_sb_eq=adpe,\n",
    "            pe_mj=pe,\n",
    "            wcf_liters=wcf,\n",
    "            usage_gwp=usage_gwp,\n",
    "            embodied_gwp=embodied_gwp,\n",
    "            tokens_processed=self.token_count if self.token_count > 0 else None,\n",
    "            latency_per_token_ms=latency_per_token,\n",
    "        )\n",
    "        \n",
    "        self.benchmark.add_result(metrics)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXEMPLE D'UTILISATION AVEC TON PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Exemple d'intégration complète dans ton pipeline de matching\n",
    "    \"\"\"\n",
    "    \n",
    "    bench = HybridCarbonBenchmark(\n",
    "        country_code=\"FRA\",\n",
    "        project_name=\"medical_nomenclature_matching\"\n",
    "    )\n",
    "    \n",
    "    # 1) spaCy (classique)\n",
    "    print(\"📊 Mesure spaCy français...\")\n",
    "    with bench.track_classic(\"spaCy_fr_core_news_md\"):\n",
    "        import spacy\n",
    "        nlp_fr = spacy.load(\"fr_core_news_md\", disable=[\"ner\", \"parser\"])\n",
    "        # texts_fr = [\"exemple\"] * 1000\n",
    "        # docs = list(nlp_fr.pipe(texts_fr, batch_size=256))\n",
    "    \n",
    "    # 2) spaCy anglais (classique)\n",
    "    print(\"📊 Mesure spaCy anglais...\")\n",
    "    with bench.track_classic(\"spaCy_en_core_web_sm\"):\n",
    "        nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "        # docs = list(nlp_en.pipe(texts_en, batch_size=256))\n",
    "    \n",
    "    # 3) Traduction (LLM)\n",
    "    print(\"📊 Mesure traduction...\")\n",
    "    tracker_translation = bench.track_llm(\n",
    "        model_name=\"Helsinki_opus-mt-fr-en\",\n",
    "        provider=\"huggingface\",\n",
    "        model_id=\"Helsinki-NLP/opus-mt-fr-en\",\n",
    "        electricity_mix_zone=\"FRA\"\n",
    "    )\n",
    "    \n",
    "    with tracker_translation:\n",
    "        from transformers import pipeline\n",
    "        translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=0)\n",
    "        # texts_fr = [\"exemple\"] * 1200\n",
    "        # outputs = translator(texts_fr, batch_size=16, truncation=True)\n",
    "        \n",
    "        # Compter les tokens générés (approximation)\n",
    "        # token_count = sum(len(o[\"translation_text\"].split()) for o in outputs)\n",
    "        token_count = 1200 * 15  # Approximation : 15 tokens/texte\n",
    "        tracker_translation.set_token_count(token_count)\n",
    "    \n",
    "    # 4) Embeddings (LLM)\n",
    "    print(\"📊 Mesure embeddings...\")\n",
    "    tracker_embeddings = bench.track_llm(\n",
    "        model_name=\"S-PubMedBert-MS-MARCO\",\n",
    "        provider=\"huggingface\",\n",
    "        model_id=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        electricity_mix_zone=\"FRA\"\n",
    "    )\n",
    "    \n",
    "    with tracker_embeddings:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model_emb = SentenceTransformer(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "        # texts = [\"exemple\"] * 6200\n",
    "        # embeddings = model_emb.encode(texts, normalize_embeddings=True, batch_size=64)\n",
    "        \n",
    "        # Approximation tokens (BERT = ~1.3 token/mot)\n",
    "        token_count = 6200 * 10 * 1.3  # 6200 textes × 10 mots × 1.3\n",
    "        tracker_embeddings.set_token_count(int(token_count))\n",
    "    \n",
    "    # 5) TF-IDF + BM25 (classique)\n",
    "    print(\"📊 Mesure TF-IDF + BM25...\")\n",
    "    with bench.track_classic(\"TF-IDF_BM25_Reranking\"):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        # vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        # tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Ton code BM25 + reranking\n",
    "        pass\n",
    "    \n",
    "    # Résultats\n",
    "    bench.print_summary()\n",
    "    bench.save_results(\"carbon_footprint_hybrid_report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b23bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:04:08] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:04:08] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:04:08] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure spaCy français...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:04:14] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:04:14] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:04:14] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure spaCy anglais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:04:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:04:19] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:04:19] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure traduction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n",
      "Could not find model `Helsinki-NLP/opus-mt-fr-en` for huggingface provider. For further information visit https://ecologits.ai/tutorial/warnings_and_errors/#model-not-registered\n",
      "[codecarbon WARNING @ 06:04:24] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:04:24] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:04:24] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find model `pritamdeka/S-PubMedBert-MS-MARCO` for huggingface provider. For further information visit https://ecologits.ai/tutorial/warnings_and_errors/#model-not-registered\n",
      "[codecarbon WARNING @ 06:04:30] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:04:30] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:04:30] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure TF-IDF + BM25...\n",
      "\n",
      "====================================================================================================\n",
      "📊 RAPPORT CARBONE HYBRIDE - Projet: medical_nomenclature_matching\n",
      "====================================================================================================\n",
      "\n",
      "🔬 Modèles classiques: 3\n",
      "🤖 Modèles LLM: 2\n",
      "\n",
      "⚡ Énergie totale (CodeCarbon): 0.000275 kWh\n",
      "🏭 CO2 total (CodeCarbon): 0.000015 kg\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Détail par modèle:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                Modèle    Type  Durée (s)  Énergie (kWh)  CO2 CodeCarbon (kg)\n",
      " spaCy_fr_core_news_md classic       5.72       0.000072             0.000004\n",
      "  spaCy_en_core_web_sm classic       4.72       0.000035             0.000002\n",
      "Helsinki_opus-mt-fr-en     llm       5.18       0.000052             0.000003\n",
      " S-PubMedBert-MS-MARCO     llm       6.37       0.000097             0.000005\n",
      " TF-IDF_BM25_Reranking classic       4.28       0.000019             0.000001\n",
      "\n",
      "====================================================================================================\n",
      "🌳 ÉQUIVALENCES CARBONE\n",
      "====================================================================================================\n",
      "• 0.0 km en voiture\n",
      "• 0.00 arbres pendant 1 an pour compenser\n",
      "• 23 charges de smartphone\n",
      "✅ Résultats sauvegardés : carbon_footprint_hybrid_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INTÉGRATION ECOLOGITS + CODECARBON\n",
    "===================================\n",
    "\n",
    "Combine les deux outils pour une analyse complète :\n",
    "- CodeCarbon : mesures générales (spaCy, TF-IDF, BM25)\n",
    "- EcoLogits : métriques détaillées pour LLMs (traduction, embeddings)\n",
    "\n",
    "Installation : pip install ecologits codecarbon\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "try:\n",
    "    from ecologits.tracers.utils import llm_impacts\n",
    "    ECOLOGITS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ECOLOGITS_AVAILABLE = False\n",
    "    print(\"⚠️ EcoLogits non disponible. Installez avec : pip install ecologits\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhancedModelMetrics:\n",
    "    \"\"\"Métriques enrichies combinant CodeCarbon + EcoLogits\"\"\"\n",
    "    model_name: str\n",
    "    model_type: str  # \"llm\" ou \"classic\"\n",
    "    \n",
    "    # Métriques communes\n",
    "    duration_s: float\n",
    "    energy_kwh: float\n",
    "    \n",
    "    # Métriques CodeCarbon (tous modèles)\n",
    "    co2_kg: float\n",
    "    cpu_power_w: float = 0.0\n",
    "    gpu_power_w: float = 0.0\n",
    "    ram_power_w: float = 0.0\n",
    "    \n",
    "    # Métriques EcoLogits (LLMs uniquement)\n",
    "    gwp_kg_co2eq: Optional[float] = None      # Global Warming Potential\n",
    "    adpe_kg_sb_eq: Optional[float] = None     # Abiotic Depletion (métaux)\n",
    "    pe_mj: Optional[float] = None             # Primary Energy\n",
    "    wcf_liters: Optional[float] = None        # Water Consumption\n",
    "    usage_gwp: Optional[float] = None         # GWP usage seulement\n",
    "    embodied_gwp: Optional[float] = None      # GWP embodied seulement\n",
    "    \n",
    "    # Métadonnées LLM\n",
    "    tokens_processed: Optional[int] = None\n",
    "    latency_per_token_ms: Optional[float] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        base = {\n",
    "            \"Modèle\": self.model_name,\n",
    "            \"Type\": self.model_type,\n",
    "            \"Durée (s)\": round(self.duration_s, 2),\n",
    "            \"Énergie (kWh)\": round(self.energy_kwh, 6),\n",
    "            \"CO2 CodeCarbon (kg)\": round(self.co2_kg, 6),\n",
    "        }\n",
    "        \n",
    "        # Ajout métriques EcoLogits si disponibles\n",
    "        if self.model_type == \"llm\" and self.gwp_kg_co2eq is not None:\n",
    "            base.update({\n",
    "                \"GWP total (kg CO2eq)\": round(self.gwp_kg_co2eq, 6),\n",
    "                \"GWP usage (kg)\": round(self.usage_gwp or 0, 6),\n",
    "                \"GWP embodied (kg)\": round(self.embodied_gwp or 0, 6),\n",
    "                \"ADPe (kg Sb eq)\": round(self.adpe_kg_sb_eq or 0, 9),\n",
    "                \"Énergie primaire (MJ)\": round(self.pe_mj or 0, 3),\n",
    "                \"Eau (litres)\": round(self.wcf_liters or 0, 3),\n",
    "                \"Tokens traités\": self.tokens_processed or 0,\n",
    "                \"Latence/token (ms)\": round(self.latency_per_token_ms or 0, 2),\n",
    "            })\n",
    "        \n",
    "        return base\n",
    "\n",
    "\n",
    "class HybridCarbonBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark hybride utilisant CodeCarbon ET EcoLogits\n",
    "    \n",
    "    Usage:\n",
    "        bench = HybridCarbonBenchmark()\n",
    "        \n",
    "        # Modèle classique (spaCy)\n",
    "        with bench.track_classic(\"spacy_fr\"):\n",
    "            nlp = spacy.load(\"fr_core_news_md\")\n",
    "            docs = list(nlp.pipe(texts))\n",
    "        \n",
    "        # Modèle LLM (traduction)\n",
    "        with bench.track_llm(\"translation\", provider=\"huggingface\", model=\"Helsinki-NLP/opus-mt-fr-en\"):\n",
    "            outputs = translator(texts)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, country_code: str = \"FRA\", project_name: str = \"medical_matching\"):\n",
    "        self.country_code = country_code\n",
    "        self.project_name = project_name\n",
    "        self.results: List[EnhancedModelMetrics] = []\n",
    "        \n",
    "        if not ECOLOGITS_AVAILABLE:\n",
    "            print(\"⚠️ EcoLogits non disponible - métriques limitées à CodeCarbon\")\n",
    "    \n",
    "    def track_classic(self, model_name: str):\n",
    "        \"\"\"Track un modèle classique (spaCy, sklearn, etc.) avec CodeCarbon\"\"\"\n",
    "        return _ClassicModelTracker(self, model_name)\n",
    "    \n",
    "    def track_llm(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        provider: str = \"huggingface\",\n",
    "        model_id: str = None,\n",
    "        electricity_mix_zone: str = None\n",
    "    ):\n",
    "        \"\"\"Track un LLM avec CodeCarbon + EcoLogits\"\"\"\n",
    "        return _LLMTracker(self, model_name, provider, model_id, electricity_mix_zone)\n",
    "    \n",
    "    def add_result(self, metrics: EnhancedModelMetrics):\n",
    "        self.results.append(metrics)\n",
    "    \n",
    "    def get_dataframe(self) -> pd.DataFrame:\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame([r.to_dict() for r in self.results])\n",
    "    \n",
    "    def print_summary(self):\n",
    "        if not self.results:\n",
    "            print(\"Aucune mesure disponible\")\n",
    "            return\n",
    "        \n",
    "        df = self.get_dataframe()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"📊 RAPPORT CARBONE HYBRIDE - Projet: {self.project_name}\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Séparation LLM vs Classic\n",
    "        llm_results = [r for r in self.results if r.model_type == \"llm\"]\n",
    "        classic_results = [r for r in self.results if r.model_type == \"classic\"]\n",
    "        \n",
    "        print(f\"\\n🔬 Modèles classiques: {len(classic_results)}\")\n",
    "        print(f\"🤖 Modèles LLM: {len(llm_results)}\")\n",
    "        \n",
    "        # Total CodeCarbon\n",
    "        total_co2_cc = sum(r.co2_kg for r in self.results)\n",
    "        total_energy = sum(r.energy_kwh for r in self.results)\n",
    "        \n",
    "        print(f\"\\n⚡ Énergie totale (CodeCarbon): {total_energy:.6f} kWh\")\n",
    "        print(f\"🏭 CO2 total (CodeCarbon): {total_co2_cc:.6f} kg\")\n",
    "        \n",
    "        # Si EcoLogits disponible, afficher métriques enrichies\n",
    "        if llm_results and llm_results[0].gwp_kg_co2eq is not None:\n",
    "            total_gwp = sum(r.gwp_kg_co2eq or 0 for r in llm_results)\n",
    "            total_adpe = sum(r.adpe_kg_sb_eq or 0 for r in llm_results)\n",
    "            total_pe = sum(r.pe_mj or 0 for r in llm_results)\n",
    "            total_water = sum(r.wcf_liters or 0 for r in llm_results)\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*100)\n",
    "            print(\"🌍 MÉTRIQUES ENRICHIES ECOLOGITS (LLMs uniquement)\")\n",
    "            print(\"-\"*100)\n",
    "            print(f\"• GWP total: {total_gwp:.6f} kg CO2eq\")\n",
    "            print(f\"  ├─ Usage: {sum(r.usage_gwp or 0 for r in llm_results):.6f} kg\")\n",
    "            print(f\"  └─ Embodied: {sum(r.embodied_gwp or 0 for r in llm_results):.6f} kg\")\n",
    "            print(f\"• ADPe (épuisement métaux): {total_adpe:.9f} kg Sb eq\")\n",
    "            print(f\"• Énergie primaire: {total_pe:.3f} MJ\")\n",
    "            print(f\"• Consommation d'eau: {total_water:.3f} litres\")\n",
    "            \n",
    "            # Équivalences eau\n",
    "            print(f\"\\n💧 Équivalent eau:\")\n",
    "            print(f\"  • {total_water / 0.25:.0f} verres d'eau (250ml)\")\n",
    "            print(f\"  • {total_water / 8:.1f} douches (8L/min pendant 1min)\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"Détail par modèle:\")\n",
    "        print(\"-\"*100)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Équivalences carbone\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"🌳 ÉQUIVALENCES CARBONE\")\n",
    "        print(\"=\"*100)\n",
    "        km_voiture = total_co2_cc / 0.12\n",
    "        arbres_an = total_co2_cc / 21\n",
    "        smartphones = total_energy * 1000 / 0.012\n",
    "        \n",
    "        print(f\"• {km_voiture:.1f} km en voiture\")\n",
    "        print(f\"• {arbres_an:.2f} arbres pendant 1 an pour compenser\")\n",
    "        print(f\"• {smartphones:.0f} charges de smartphone\")\n",
    "    \n",
    "    def save_results(self, filepath: str):\n",
    "        df = self.get_dataframe()\n",
    "        if df.empty:\n",
    "            print(\"Aucun résultat à sauvegarder\")\n",
    "            return\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Métriques complètes\", index=False)\n",
    "            \n",
    "            # Onglet comparaison LLM vs Classic\n",
    "            comparison = pd.DataFrame({\n",
    "                \"Type\": [\"Modèles classiques\", \"Modèles LLM\", \"TOTAL\"],\n",
    "                \"Nombre\": [\n",
    "                    len([r for r in self.results if r.model_type == \"classic\"]),\n",
    "                    len([r for r in self.results if r.model_type == \"llm\"]),\n",
    "                    len(self.results)\n",
    "                ],\n",
    "                \"CO2 (kg)\": [\n",
    "                    sum(r.co2_kg for r in self.results if r.model_type == \"classic\"),\n",
    "                    sum(r.co2_kg for r in self.results if r.model_type == \"llm\"),\n",
    "                    sum(r.co2_kg for r in self.results)\n",
    "                ],\n",
    "                \"Énergie (kWh)\": [\n",
    "                    sum(r.energy_kwh for r in self.results if r.model_type == \"classic\"),\n",
    "                    sum(r.energy_kwh for r in self.results if r.model_type == \"llm\"),\n",
    "                    sum(r.energy_kwh for r in self.results)\n",
    "                ]\n",
    "            })\n",
    "            comparison.to_excel(writer, sheet_name=\"LLM vs Classic\", index=False)\n",
    "        \n",
    "        print(f\"✅ Résultats sauvegardés : {filepath}\")\n",
    "\n",
    "\n",
    "class _ClassicModelTracker:\n",
    "    \"\"\"Tracker pour modèles classiques (CodeCarbon uniquement)\"\"\"\n",
    "    \n",
    "    def __init__(self, benchmark: HybridCarbonBenchmark, model_name: str):\n",
    "        self.benchmark = benchmark\n",
    "        self.model_name = model_name\n",
    "        self.tracker = None\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        # CodeCarbon 3.x utilise country_2letter_iso_code (pas country_iso_code)\n",
    "        country_code_2letter = self.benchmark.country_code[:2] if len(self.benchmark.country_code) > 2 else self.benchmark.country_code\n",
    "        self.tracker = EmissionsTracker(\n",
    "            project_name=self.benchmark.project_name,\n",
    "            # country_2letter_iso_code=country_code_2letter,\n",
    "            log_level=\"warning\",\n",
    "        )\n",
    "        self.tracker.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        emissions = self.tracker.stop()\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        metrics = EnhancedModelMetrics(\n",
    "            model_name=self.model_name,\n",
    "            model_type=\"classic\",\n",
    "            duration_s=duration,\n",
    "            energy_kwh=self.tracker._total_energy.kWh if hasattr(self.tracker, '_total_energy') else 0,\n",
    "            co2_kg=emissions if emissions else 0,\n",
    "            cpu_power_w=self.tracker._cpu_power.W if hasattr(self.tracker, '_cpu_power') else 0,\n",
    "            gpu_power_w=self.tracker._gpu_power.W if hasattr(self.tracker, '_gpu_power') else 0,\n",
    "            ram_power_w=self.tracker._ram_power.W if hasattr(self.tracker, '_ram_power') else 0,\n",
    "        )\n",
    "        \n",
    "        self.benchmark.add_result(metrics)\n",
    "\n",
    "\n",
    "class _LLMTracker:\n",
    "    \"\"\"Tracker pour LLMs (CodeCarbon + EcoLogits)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        benchmark: HybridCarbonBenchmark, \n",
    "        model_name: str,\n",
    "        provider: str,\n",
    "        model_id: str,\n",
    "        electricity_mix_zone: str\n",
    "    ):\n",
    "        self.benchmark = benchmark\n",
    "        self.model_name = model_name\n",
    "        self.provider = provider\n",
    "        self.model_id = model_id or model_name\n",
    "        self.electricity_mix_zone = electricity_mix_zone or benchmark.country_code\n",
    "        self.tracker = None\n",
    "        self.start_time = None\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.tracker = EmissionsTracker(\n",
    "            project_name=self.benchmark.project_name,\n",
    "            # country_iso_code=self.benchmark.country_code,\n",
    "            log_level=\"WARNING\",\n",
    "        )\n",
    "        self.tracker.start()\n",
    "        return self\n",
    "    \n",
    "    def set_token_count(self, count: int):\n",
    "        \"\"\"Permet de définir le nombre de tokens traités\"\"\"\n",
    "        self.token_count = count\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        emissions_cc = self.tracker.stop()\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        # Métriques CodeCarbon\n",
    "        energy_kwh = self.tracker._total_energy.kWh if hasattr(self.tracker, '_total_energy') else 0\n",
    "        \n",
    "        # Métriques EcoLogits (si disponible)\n",
    "        gwp = adpe = pe = wcf = usage_gwp = embodied_gwp = None\n",
    "        latency_per_token = None\n",
    "        \n",
    "        if ECOLOGITS_AVAILABLE and self.token_count > 0:\n",
    "            try:\n",
    "                impacts = llm_impacts(\n",
    "                    provider=self.provider,\n",
    "                    model_name=self.model_id,\n",
    "                    output_token_count=self.token_count,\n",
    "                    request_latency=duration,\n",
    "                    electricity_mix_zone=self.electricity_mix_zone,\n",
    "                )\n",
    "                \n",
    "                if impacts.gwp:\n",
    "                    gwp = impacts.gwp.value\n",
    "                    usage_gwp = impacts.usage.gwp.value if impacts.usage and impacts.usage.gwp else None\n",
    "                    embodied_gwp = impacts.embodied.gwp.value if impacts.embodied and impacts.embodied.gwp else None\n",
    "                \n",
    "                adpe = impacts.adpe.value if impacts.adpe else None\n",
    "                pe = impacts.pe.value if impacts.pe else None\n",
    "                wcf = impacts.wcf.value if impacts.wcf else None\n",
    "                latency_per_token = (duration / self.token_count) * 1000  # ms\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur EcoLogits pour {self.model_name}: {e}\")\n",
    "        \n",
    "        metrics = EnhancedModelMetrics(\n",
    "            model_name=self.model_name,\n",
    "            model_type=\"llm\",\n",
    "            duration_s=duration,\n",
    "            energy_kwh=energy_kwh,\n",
    "            co2_kg=emissions_cc if emissions_cc else 0,\n",
    "            gwp_kg_co2eq=gwp,\n",
    "            adpe_kg_sb_eq=adpe,\n",
    "            pe_mj=pe,\n",
    "            wcf_liters=wcf,\n",
    "            usage_gwp=usage_gwp,\n",
    "            embodied_gwp=embodied_gwp,\n",
    "            tokens_processed=self.token_count if self.token_count > 0 else None,\n",
    "            latency_per_token_ms=latency_per_token,\n",
    "        )\n",
    "        \n",
    "        self.benchmark.add_result(metrics)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXEMPLE D'UTILISATION AVEC TON PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Exemple d'intégration complète dans ton pipeline de matching\n",
    "    \"\"\"\n",
    "    \n",
    "    bench = HybridCarbonBenchmark(\n",
    "        country_code=\"FRA\",\n",
    "        project_name=\"medical_nomenclature_matching\"\n",
    "    )\n",
    "    \n",
    "    # 1) spaCy (classique)\n",
    "    print(\"📊 Mesure spaCy français...\")\n",
    "    with bench.track_classic(\"spaCy_fr_core_news_md\"):\n",
    "        import spacy\n",
    "        nlp_fr = spacy.load(\"fr_core_news_md\", disable=[\"ner\", \"parser\"])\n",
    "        # texts_fr = [\"exemple\"] * 1000\n",
    "        # docs = list(nlp_fr.pipe(texts_fr, batch_size=256))\n",
    "    \n",
    "    # 2) spaCy anglais (classique)\n",
    "    print(\"📊 Mesure spaCy anglais...\")\n",
    "    with bench.track_classic(\"spaCy_en_core_web_sm\"):\n",
    "        nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "        # docs = list(nlp_en.pipe(texts_en, batch_size=256))\n",
    "    \n",
    "    # 3) Traduction (LLM)\n",
    "    print(\"📊 Mesure traduction...\")\n",
    "    tracker_translation = bench.track_llm(\n",
    "        model_name=\"Helsinki_opus-mt-fr-en\",\n",
    "        provider=\"huggingface\",\n",
    "        model_id=\"Helsinki-NLP/opus-mt-fr-en\",\n",
    "        electricity_mix_zone=\"FRA\"\n",
    "    )\n",
    "    \n",
    "    with tracker_translation:\n",
    "        from transformers import pipeline\n",
    "        translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=0)\n",
    "        # texts_fr = [\"exemple\"] * 1200\n",
    "        # outputs = translator(texts_fr, batch_size=16, truncation=True)\n",
    "        \n",
    "        # Compter les tokens générés (approximation)\n",
    "        # token_count = sum(len(o[\"translation_text\"].split()) for o in outputs)\n",
    "        token_count = 1200 * 15  # Approximation : 15 tokens/texte\n",
    "        tracker_translation.set_token_count(token_count)\n",
    "    \n",
    "    # 4) Embeddings (LLM)\n",
    "    print(\"📊 Mesure embeddings...\")\n",
    "    tracker_embeddings = bench.track_llm(\n",
    "        model_name=\"S-PubMedBert-MS-MARCO\",\n",
    "        provider=\"huggingface\",\n",
    "        model_id=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        electricity_mix_zone=\"FRA\"\n",
    "    )\n",
    "    \n",
    "    with tracker_embeddings:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model_emb = SentenceTransformer(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "        # texts = [\"exemple\"] * 6200\n",
    "        # embeddings = model_emb.encode(texts, normalize_embeddings=True, batch_size=64)\n",
    "        \n",
    "        # Approximation tokens (BERT = ~1.3 token/mot)\n",
    "        token_count = 6200 * 10 * 1.3  # 6200 textes × 10 mots × 1.3\n",
    "        tracker_embeddings.set_token_count(int(token_count))\n",
    "    \n",
    "    # 5) TF-IDF + BM25 (classique)\n",
    "    print(\"📊 Mesure TF-IDF + BM25...\")\n",
    "    with bench.track_classic(\"TF-IDF_BM25_Reranking\"):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        # vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        # tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Ton code BM25 + reranking\n",
    "        pass\n",
    "    \n",
    "    # Résultats\n",
    "    bench.print_summary()\n",
    "    bench.save_results(\"carbon_footprint_hybrid_report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "317ec77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import codecarbon\n",
    "print(codecarbon.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
