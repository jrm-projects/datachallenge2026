{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d75ee",
   "metadata": {},
   "source": [
    "# Axe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1beb3",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4437d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7153e13",
   "metadata": {},
   "source": [
    "### 1) Processing du df composant médical - Emission carbonne (kgCO2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees\n",
      "0    pansements composites                    0.9400              m2\n",
      "1           Sonde urinaire                   51.9034      parProduit\n",
      "2  Set de sondage urinaire                   89.4064      parProduit\n",
      "3      Collecteur de jambe                  250.7186      parProduit\n",
      "4       Collecteur de nuit                  676.6522      parProduit\n",
      "{'pansements composites': 2880.7506438973483, 'Sonde urinaire': 2.3058391190603773, 'Set de sondage urinaire': 1301.0278534529064, 'Collecteur de jambe': 1871.2095479103848, 'Collecteur de nuit': 2265.768898794339, 'Etuis péniens': 1088.7167868279912, 'Poche pour stomie': 3.131259971503482, 'Support pour stomie': 2959.0238083770532, 'Changes complets': 2375.889528275952, 'Slips absorbants': 250.72891706628525, 'Protections absorbantes': 1887.063988630245, 'Couches droites': 1329.456550312355, 'Alèses': 1383.9846297425656, 'pansement': 2495.056711895948, 'uteruscopes': 2579.2114069150794, 'instrument usage unique': 1233.5956502107485, 'bronchoscope': 2584.057077104993, 'verre lunettes': 570.6534648859387, 'Scanners': 1494.6674379613792, 'IRM (dont cage de Faraday. hélium)': 1479.9295216240887, 'Caméras à scintillation': 399.9716995350735, 'Tomographes à émission/ caméras à positons': 1949.3421425722856, 'Salles de radiologie': 2073.3922006142684, 'Appareils de mammographie': 568.0886777818828, 'Appareils de radiographie dentaire standard': 1571.6806541816616, 'Appareils de radiographie dentaire panoramique': 1792.2833323467103, 'Echographes portables': 2530.639912909539, 'Echographes fixes': 2426.4950561912756, 'Arceaux mobiles de radioscopie': 2592.063547762132, 'Statifs vasculaires avec arceau': 663.5845948900616, 'DAE': 1897.2515853423176, \"Générateurs d'hémodialyse\": 983.7099696401053, 'Equipements de radiothérapie': 1520.2436884291217, \"Ventilateurs d'anesthésie et de réanimation\": 197.9303483872975, 'Robots chirurgicaux': 1319.1239098347266, 'Pompes à perfusion': 596.6801935342754, 'Pousse-seringues': 1453.2358761107366, 'Pompes à nutrition (entérale ou parentérale)': 2705.902467790883, 'Autoclaves': 605.179419391841, 'Laveurs désinfecteurs': 2099.402660695002, 'Colonnes d’endoscopie et de coelioscopie': 1135.8599146287866, 'Moniteurs multi-paramétriques (ECG/PNI/SPO2. etc.)': 1912.330303059016, 'Aspirateurs de mucosités': 935.9956224870981, 'complement alimentaire': 1076.3137946700936}\n",
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees  \\\n",
      "0    pansements composites                    0.9400              m2   \n",
      "1           Sonde urinaire                   51.9034      parProduit   \n",
      "2  Set de sondage urinaire                   89.4064      parProduit   \n",
      "3      Collecteur de jambe                  250.7186      parProduit   \n",
      "4       Collecteur de nuit                  676.6522      parProduit   \n",
      "\n",
      "   Emission_carbonne_total_des_produits_kgCO2e  \n",
      "0                                 2.820000e+03  \n",
      "1                                 1.196809e+02  \n",
      "2                                 1.163202e+05  \n",
      "3                                 4.691470e+05  \n",
      "4                                 1.533138e+06  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def concat_xlsx_from_folder(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parcourt récursivement un dossier git et concatène verticalement\n",
    "    toutes les tables issues des fichiers .xlsx.\n",
    "\n",
    "    Hypothèses :\n",
    "    - chaque fichier .xlsx contient une table avec exactement 2 colonnes\n",
    "    - la première ligne correspond aux labels et est ignorée\n",
    "    - colonne A : produit\n",
    "    - colonne B : Emission_kgCO2e_unitaire\n",
    "    \"\"\"\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    all_rows = []\n",
    "\n",
    "    for file in folder.rglob(\"*.xlsx\"):\n",
    "        df = pd.read_excel(file, header=0)\n",
    "\n",
    "        df = df.iloc[:, :2]\n",
    "        df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "        filename = file.stem\n",
    "        if filename.endswith(\"_parProduit\"):\n",
    "            df[\"type_de_donnees\"] = \"parProduit\"\n",
    "        elif filename.endswith(\"_m2\"):\n",
    "            df[\"type_de_donnees\"] = \"m2\"\n",
    "        elif filename.endswith(\"_parKG\"):\n",
    "            df[\"type_de_donnees\"] = \"parKG\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_xlsx_from_folders(list_paths: list[str | Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatène verticalement les tables issues de plusieurs dossiers.\n",
    "\n",
    "    Paramètre\n",
    "    ----------\n",
    "    list_paths : list[str | Path]\n",
    "        Liste de chemins vers des dossiers contenant des fichiers .xlsx\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    DataFrame avec les colonnes :\n",
    "    - produit\n",
    "    - Emission_kgCO2e_unitaire\n",
    "    - type_de_donnees\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for folder_path in list_paths:\n",
    "        folder = Path(folder_path)\n",
    "\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Dossier introuvable : {folder}\")\n",
    "\n",
    "        xlsx_files = list(folder.rglob(\"*.xlsx\"))\n",
    "        if not xlsx_files:\n",
    "            raise ValueError(f\"Aucun fichier .xlsx trouvé dans {folder}\")\n",
    "\n",
    "        for file in xlsx_files:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "            name = file.stem\n",
    "            if name.endswith(\"_parProduit\"):\n",
    "                df[\"type_de_donnees\"] = \"parProduit\"\n",
    "            elif name.endswith(\"_m2\"):\n",
    "                df[\"type_de_donnees\"] = \"m2\"\n",
    "            elif name.endswith(\"_parKG\"):\n",
    "                df[\"type_de_donnees\"] = \"parKG\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def random_value_dict(\n",
    "    df: pd.DataFrame,\n",
    "    nom_col: str = \"produit\",\n",
    "    nom_to_ignore: list | None = None,\n",
    "    min_value: float = 1.0,\n",
    "    max_value: float = 3000.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Construit un dictionnaire :\n",
    "    - clés : valeurs uniques de df[nom_col]\n",
    "    - valeurs : nombre aléatoire strictement > 1\n",
    "      tiré dans [min_value, max_value]\n",
    "    \"\"\"\n",
    "\n",
    "    if nom_to_ignore is None:\n",
    "        nom_to_ignore = []\n",
    "\n",
    "    if min_value <= 1:\n",
    "        min_value = 1.000001\n",
    "\n",
    "    uniques = df[nom_col].dropna().unique()\n",
    "\n",
    "    return {\n",
    "        val: random.uniform(min_value, max_value)\n",
    "        for val in uniques\n",
    "        if val not in nom_to_ignore\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_emission_hopital(\n",
    "    df: pd.DataFrame,\n",
    "    type_de_donnees: str = \"type_de_donnees\",\n",
    "    dict_m2: dict = {\"pansements composites\": (3 * 10**2, 10)},\n",
    "    dict_parKG: dict = {\"instrument usage unique\": 1000, \"complement alimentaire\":10000},\n",
    "    dict_nb_parProduit: dict | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute la colonne Emission_carbonne_total_des_produits_kgCO2e selon\n",
    "    le type de données associé à chaque produit.\n",
    "    \"\"\"\n",
    "\n",
    "    if dict_nb_parProduit is None:\n",
    "        dict_nb_parProduit = {}\n",
    "\n",
    "    def compute_row(row):\n",
    "        produit = row[\"produit\"]\n",
    "        emission_unit = row[\"Emission_kgCO2e_unitaire\"]\n",
    "        t = row[type_de_donnees]\n",
    "\n",
    "        if t == \"parProduit\":\n",
    "            return emission_unit * dict_nb_parProduit.get(produit, np.nan)\n",
    "\n",
    "        if t == \"m2\":\n",
    "            longueur, largeur = dict_m2.get(produit, (np.nan, np.nan))\n",
    "            return emission_unit * longueur * largeur\n",
    "\n",
    "        if t == \"parKG\":\n",
    "            return emission_unit * dict_parKG.get(produit, np.nan)\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Emission_carbonne_total_des_produits_kgCO2e\"] = df.apply(compute_row, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 3. Construction du DataFrame\n",
    "# =========================\n",
    "\n",
    "# extract_path = \"sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# extract_path = \"C:/Users/jerem/Documents/GitHub/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "extract_path = \"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# paths = [\n",
    "#     r\"sujets\\chu\\Axe_2\\Axe_2_bdd-20260117T004817Z-1-001\\Axe_2_bdd\",\n",
    "#     r\"sujets\\chu\\Axe_2\\autre_dossier\"\n",
    "# ]\n",
    "\n",
    "df_concat = concat_xlsx_from_folder(extract_path)\n",
    "# df_concat = concat_xlsx_from_folders(paths)\n",
    "\n",
    "print(df_concat.head())\n",
    "\n",
    "# =========================\n",
    "# 4. Dictionnaires exemples\n",
    "# =========================\n",
    "\n",
    "dict_nb_parProduit = random_value_dict(df_concat)\n",
    "\n",
    "print(dict_nb_parProduit)\n",
    "\n",
    "# =========================\n",
    "# 5. Calcul des émissions\n",
    "# =========================\n",
    "\n",
    "df_final = compute_emission_hopital(\n",
    "    df_concat,\n",
    "    dict_nb_parProduit=dict_nb_parProduit\n",
    ")\n",
    "\n",
    "print(df_final.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7ee391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons sur la colonne 'produit' :\n",
      "Empty DataFrame\n",
      "Columns: [produit, Emission_kgCO2e_unitaire, type_de_donnees, Emission_carbonne_total_des_produits_kgCO2e]\n",
      "Index: []\n",
      "\n",
      "Valeurs répétées dans 'produit' :\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Doublons sur une variable particulière (ex: 'email')\n",
    "colonne = 'produit'\n",
    "\n",
    "# Trouver les valeurs dupliquées dans cette colonne\n",
    "valeurs_doublons = df_final[df_final.duplicated(subset=[colonne], keep=False)]\n",
    "\n",
    "# Afficher les doublons triés pour mieux voir\n",
    "doublons_tries = valeurs_doublons.sort_values(colonne)\n",
    "print(f\"Doublons sur la colonne '{colonne}' :\")\n",
    "print(doublons_tries)\n",
    "\n",
    "# Voir les valeurs qui se répètent\n",
    "comptage = df_final[colonne].value_counts()\n",
    "valeurs_repetees = comptage[comptage > 1]\n",
    "print(f\"\\nValeurs répétées dans '{colonne}' :\")\n",
    "print(valeurs_repetees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f786ffa",
   "metadata": {},
   "source": [
    "#### 1.1) Obtention du dataframe final et exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbd6cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_conserver = [\n",
    "    \"produit\",\n",
    "    \"Emission_kgCO2e_unitaire\",\n",
    "    \"Emission_carbonne_total_des_produits_kgCO2e\"\n",
    "]\n",
    "\n",
    "df_export = df_final[colonnes_a_conserver].copy()\n",
    "\n",
    "# =========================\n",
    "# Chemin de sortie\n",
    "# =========================\n",
    "\n",
    "output_path = Path(r\"results\\df_composant_medical_emissions_carbones.xlsx\")\n",
    "\n",
    "# Création du dossier si besoin\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Export Excel\n",
    "# =========================\n",
    "\n",
    "df_export.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a2ec7",
   "metadata": {},
   "source": [
    "### 2) Pre processing NLP des bases **df_composant_medical_emissions_carbones.xlsx** et **DISPOSITIFS_MED.xlsx** et Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28d8d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/python/lib/python3.13/site-packages (from sentence_transformers) (2.9.1)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/python/lib/python3.13/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (3.20.3)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2026.1.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (2026.1.4)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, safetensors, regex, pyyaml, joblib, hf-xet, scikit-learn, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.3 pyyaml-6.0.3 regex-2026.1.15 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 sentence_transformers-5.2.0 threadpoolctl-3.6.0 tokenizers-0.22.2 tqdm-4.67.1 transformers-4.57.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/python/lib/python3.13/site-packages (4.57.6)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/python/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2026.1.4)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from rank_bm25) (2.4.1)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: transformers in /opt/python/lib/python3.13/site-packages (4.57.6)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch in /opt/python/lib/python3.13/site-packages (2.9.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/python/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/python/lib/python3.13/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/python/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/python/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in /opt/python/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.11-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (32.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (254 kB)\n",
      "Downloading murmurhash-1.0.15-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (133 kB)\n",
      "Downloading preshed-3.0.12-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (835 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m835.9/835.9 kB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.10-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: wasabi, typing-inspection, spacy-loggers, spacy-legacy, smart-open, sentencepiece, pydantic-core, murmurhash, cymem, cloudpathlib, click, catalogue, blis, annotated-types, typer-slim, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [spacy]m21/22\u001b[0m [spacy]ic]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 click-8.3.1 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 sentencepiece-0.2.1 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3\n",
      "Collecting fr-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-md\n",
      "Successfully installed fr-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: rank-bm25 in /opt/python/lib/python3.13/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from rank-bm25) (2.4.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/python/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: tqdm in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /opt/python/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/python/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# on sup un encoding: utf-8\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "!pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "!pip install spacy transformers sentencepiece torch\n",
    "!python -m spacy download fr_core_news_md\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install rank-bm25\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210221c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'df_composant_medical_emissions_carbones.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 259\u001b[0m\n\u001b[0;32m    256\u001b[0m path_df1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_composant_medical_emissions_carbones.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m path_df2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDISPOSITIFS_MED.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 259\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(path_df1)\n\u001b[0;32m    260\u001b[0m df2_full \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(path_df2)\n\u001b[0;32m    261\u001b[0m df2 \u001b[38;5;241m=\u001b[39m keep_df2_columns(df2_full)\n",
      "File \u001b[1;32mc:\\Users\\jerem\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jerem\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jerem\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jerem\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'df_composant_medical_emissions_carbones.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    device: int = -1  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \"\"\"\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # calcul similarities restreint\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        sims[i, :] = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy()}\n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # NLP + traduction\n",
    "    translator = TranslatorFR2EN(device=-1)  # CPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "    # Matching BM25 -> Embeddings -> Top5\n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "\n",
    "    # Sauvegarde\n",
    "    match_wide.to_excel(\"MATCH_df2_vers_df1_top5.xlsx\", index=False)\n",
    "    match_wide.attrs[\"out_long\"].to_excel(\"MATCH_df2_vers_df1_top5_long.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e87b6",
   "metadata": {},
   "source": [
    "### 3) Réduction et évaluation des coûts carbones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d582a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLM, TintBERT ?\n",
    "# Métriques et graphes de consommation carbonne par inférence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
