{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d75ee",
   "metadata": {},
   "source": [
    "# Axe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1beb3",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4437d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7153e13",
   "metadata": {},
   "source": [
    "### 1) Processing du df composant médical - Emission carbonne (kgCO2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees\n",
      "0    pansements composites                    0.9400              m2\n",
      "1           Sonde urinaire                   51.9034      parProduit\n",
      "2  Set de sondage urinaire                   89.4064      parProduit\n",
      "3      Collecteur de jambe                  250.7186      parProduit\n",
      "4       Collecteur de nuit                  676.6522      parProduit\n",
      "{'pansements composites': 2880.7506438973483, 'Sonde urinaire': 2.3058391190603773, 'Set de sondage urinaire': 1301.0278534529064, 'Collecteur de jambe': 1871.2095479103848, 'Collecteur de nuit': 2265.768898794339, 'Etuis péniens': 1088.7167868279912, 'Poche pour stomie': 3.131259971503482, 'Support pour stomie': 2959.0238083770532, 'Changes complets': 2375.889528275952, 'Slips absorbants': 250.72891706628525, 'Protections absorbantes': 1887.063988630245, 'Couches droites': 1329.456550312355, 'Alèses': 1383.9846297425656, 'pansement': 2495.056711895948, 'uteruscopes': 2579.2114069150794, 'instrument usage unique': 1233.5956502107485, 'bronchoscope': 2584.057077104993, 'verre lunettes': 570.6534648859387, 'Scanners': 1494.6674379613792, 'IRM (dont cage de Faraday. hélium)': 1479.9295216240887, 'Caméras à scintillation': 399.9716995350735, 'Tomographes à émission/ caméras à positons': 1949.3421425722856, 'Salles de radiologie': 2073.3922006142684, 'Appareils de mammographie': 568.0886777818828, 'Appareils de radiographie dentaire standard': 1571.6806541816616, 'Appareils de radiographie dentaire panoramique': 1792.2833323467103, 'Echographes portables': 2530.639912909539, 'Echographes fixes': 2426.4950561912756, 'Arceaux mobiles de radioscopie': 2592.063547762132, 'Statifs vasculaires avec arceau': 663.5845948900616, 'DAE': 1897.2515853423176, \"Générateurs d'hémodialyse\": 983.7099696401053, 'Equipements de radiothérapie': 1520.2436884291217, \"Ventilateurs d'anesthésie et de réanimation\": 197.9303483872975, 'Robots chirurgicaux': 1319.1239098347266, 'Pompes à perfusion': 596.6801935342754, 'Pousse-seringues': 1453.2358761107366, 'Pompes à nutrition (entérale ou parentérale)': 2705.902467790883, 'Autoclaves': 605.179419391841, 'Laveurs désinfecteurs': 2099.402660695002, 'Colonnes d’endoscopie et de coelioscopie': 1135.8599146287866, 'Moniteurs multi-paramétriques (ECG/PNI/SPO2. etc.)': 1912.330303059016, 'Aspirateurs de mucosités': 935.9956224870981, 'complement alimentaire': 1076.3137946700936}\n",
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees  \\\n",
      "0    pansements composites                    0.9400              m2   \n",
      "1           Sonde urinaire                   51.9034      parProduit   \n",
      "2  Set de sondage urinaire                   89.4064      parProduit   \n",
      "3      Collecteur de jambe                  250.7186      parProduit   \n",
      "4       Collecteur de nuit                  676.6522      parProduit   \n",
      "\n",
      "   Emission_carbonne_total_des_produits_kgCO2e  \n",
      "0                                 2.820000e+03  \n",
      "1                                 1.196809e+02  \n",
      "2                                 1.163202e+05  \n",
      "3                                 4.691470e+05  \n",
      "4                                 1.533138e+06  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def concat_xlsx_from_folder(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parcourt récursivement un dossier git et concatène verticalement\n",
    "    toutes les tables issues des fichiers .xlsx.\n",
    "\n",
    "    Hypothèses :\n",
    "    - chaque fichier .xlsx contient une table avec exactement 2 colonnes\n",
    "    - la première ligne correspond aux labels et est ignorée\n",
    "    - colonne A : produit\n",
    "    - colonne B : Emission_kgCO2e_unitaire\n",
    "    \"\"\"\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    all_rows = []\n",
    "\n",
    "    for file in folder.rglob(\"*.xlsx\"):\n",
    "        df = pd.read_excel(file, header=0)\n",
    "\n",
    "        df = df.iloc[:, :2]\n",
    "        df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "        filename = file.stem\n",
    "        if filename.endswith(\"_parProduit\"):\n",
    "            df[\"type_de_donnees\"] = \"parProduit\"\n",
    "        elif filename.endswith(\"_m2\"):\n",
    "            df[\"type_de_donnees\"] = \"m2\"\n",
    "        elif filename.endswith(\"_parKG\"):\n",
    "            df[\"type_de_donnees\"] = \"parKG\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_xlsx_from_folders(list_paths: list[str | Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatène verticalement les tables issues de plusieurs dossiers.\n",
    "\n",
    "    Paramètre\n",
    "    ----------\n",
    "    list_paths : list[str | Path]\n",
    "        Liste de chemins vers des dossiers contenant des fichiers .xlsx\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    DataFrame avec les colonnes :\n",
    "    - produit\n",
    "    - Emission_kgCO2e_unitaire\n",
    "    - type_de_donnees\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for folder_path in list_paths:\n",
    "        folder = Path(folder_path)\n",
    "\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Dossier introuvable : {folder}\")\n",
    "\n",
    "        xlsx_files = list(folder.rglob(\"*.xlsx\"))\n",
    "        if not xlsx_files:\n",
    "            raise ValueError(f\"Aucun fichier .xlsx trouvé dans {folder}\")\n",
    "\n",
    "        for file in xlsx_files:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "            name = file.stem\n",
    "            if name.endswith(\"_parProduit\"):\n",
    "                df[\"type_de_donnees\"] = \"parProduit\"\n",
    "            elif name.endswith(\"_m2\"):\n",
    "                df[\"type_de_donnees\"] = \"m2\"\n",
    "            elif name.endswith(\"_parKG\"):\n",
    "                df[\"type_de_donnees\"] = \"parKG\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def random_value_dict(\n",
    "    df: pd.DataFrame,\n",
    "    nom_col: str = \"produit\",\n",
    "    nom_to_ignore: list | None = None,\n",
    "    min_value: float = 1.0,\n",
    "    max_value: float = 3000.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Construit un dictionnaire :\n",
    "    - clés : valeurs uniques de df[nom_col]\n",
    "    - valeurs : nombre aléatoire strictement > 1\n",
    "      tiré dans [min_value, max_value]\n",
    "    \"\"\"\n",
    "\n",
    "    if nom_to_ignore is None:\n",
    "        nom_to_ignore = []\n",
    "\n",
    "    if min_value <= 1:\n",
    "        min_value = 1.000001\n",
    "\n",
    "    uniques = df[nom_col].dropna().unique()\n",
    "\n",
    "    return {\n",
    "        val: random.uniform(min_value, max_value)\n",
    "        for val in uniques\n",
    "        if val not in nom_to_ignore\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_emission_hopital(\n",
    "    df: pd.DataFrame,\n",
    "    type_de_donnees: str = \"type_de_donnees\",\n",
    "    dict_m2: dict = {\"pansements composites\": (3 * 10**2, 10)},\n",
    "    dict_parKG: dict = {\"instrument usage unique\": 1000, \"complement alimentaire\":10000},\n",
    "    dict_nb_parProduit: dict | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute la colonne Emission_carbonne_total_des_produits_kgCO2e selon\n",
    "    le type de données associé à chaque produit.\n",
    "    \"\"\"\n",
    "\n",
    "    if dict_nb_parProduit is None:\n",
    "        dict_nb_parProduit = {}\n",
    "\n",
    "    def compute_row(row):\n",
    "        produit = row[\"produit\"]\n",
    "        emission_unit = row[\"Emission_kgCO2e_unitaire\"]\n",
    "        t = row[type_de_donnees]\n",
    "\n",
    "        if t == \"parProduit\":\n",
    "            return emission_unit * dict_nb_parProduit.get(produit, np.nan)\n",
    "\n",
    "        if t == \"m2\":\n",
    "            longueur, largeur = dict_m2.get(produit, (np.nan, np.nan))\n",
    "            return emission_unit * longueur * largeur\n",
    "\n",
    "        if t == \"parKG\":\n",
    "            return emission_unit * dict_parKG.get(produit, np.nan)\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Emission_carbonne_total_des_produits_kgCO2e\"] = df.apply(compute_row, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 3. Construction du DataFrame\n",
    "# =========================\n",
    "\n",
    "# extract_path = \"sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# extract_path = \"C:/Users/jerem/Documents/GitHub/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "extract_path = \"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# paths = [\n",
    "#     r\"sujets\\chu\\Axe_2\\Axe_2_bdd-20260117T004817Z-1-001\\Axe_2_bdd\",\n",
    "#     r\"sujets\\chu\\Axe_2\\autre_dossier\"\n",
    "# ]\n",
    "\n",
    "df_concat = concat_xlsx_from_folder(extract_path)\n",
    "# df_concat = concat_xlsx_from_folders(paths)\n",
    "\n",
    "print(df_concat.head())\n",
    "\n",
    "# =========================\n",
    "# 4. Dictionnaires exemples\n",
    "# =========================\n",
    "\n",
    "dict_nb_parProduit = random_value_dict(df_concat)\n",
    "\n",
    "print(dict_nb_parProduit)\n",
    "\n",
    "# =========================\n",
    "# 5. Calcul des émissions\n",
    "# =========================\n",
    "\n",
    "df_final = compute_emission_hopital(\n",
    "    df_concat,\n",
    "    dict_nb_parProduit=dict_nb_parProduit\n",
    ")\n",
    "\n",
    "print(df_final.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7ee391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons sur la colonne 'produit' :\n",
      "Empty DataFrame\n",
      "Columns: [produit, Emission_kgCO2e_unitaire, type_de_donnees, Emission_carbonne_total_des_produits_kgCO2e]\n",
      "Index: []\n",
      "\n",
      "Valeurs répétées dans 'produit' :\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Doublons sur une variable particulière (ex: 'email')\n",
    "colonne = 'produit'\n",
    "\n",
    "# Trouver les valeurs dupliquées dans cette colonne\n",
    "valeurs_doublons = df_final[df_final.duplicated(subset=[colonne], keep=False)]\n",
    "\n",
    "# Afficher les doublons triés pour mieux voir\n",
    "doublons_tries = valeurs_doublons.sort_values(colonne)\n",
    "print(f\"Doublons sur la colonne '{colonne}' :\")\n",
    "print(doublons_tries)\n",
    "\n",
    "# Voir les valeurs qui se répètent\n",
    "comptage = df_final[colonne].value_counts()\n",
    "valeurs_repetees = comptage[comptage > 1]\n",
    "print(f\"\\nValeurs répétées dans '{colonne}' :\")\n",
    "print(valeurs_repetees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f786ffa",
   "metadata": {},
   "source": [
    "#### 1.1) Obtention du dataframe final et exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbd6cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_conserver = [\n",
    "    \"produit\",\n",
    "    \"Emission_kgCO2e_unitaire\",\n",
    "    \"Emission_carbonne_total_des_produits_kgCO2e\"\n",
    "]\n",
    "\n",
    "df_export = df_final[colonnes_a_conserver].copy()\n",
    "\n",
    "# =========================\n",
    "# Chemin de sortie\n",
    "# =========================\n",
    "\n",
    "output_path = Path(r\"results\\df_composant_medical_emissions_carbones.xlsx\")\n",
    "\n",
    "# Création du dossier si besoin\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Export Excel\n",
    "# =========================\n",
    "\n",
    "df_export.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a2ec7",
   "metadata": {},
   "source": [
    "### 2) Pre processing NLP des bases **df_composant_medical_emissions_carbones.xlsx** et **DISPOSITIFS_MED.xlsx** et Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d8d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/python/lib/python3.13/site-packages (from sentence_transformers) (2.9.1)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Downloading scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/python/lib/python3.13/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (3.20.3)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2026.1.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence_transformers) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence_transformers) (2026.1.4)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading scikit_learn-1.8.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, safetensors, regex, pyyaml, joblib, hf-xet, scikit-learn, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [sentence_transformers]ence_transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.3 pyyaml-6.0.3 regex-2026.1.15 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.17.0 sentence_transformers-5.2.0 threadpoolctl-3.6.0 tokenizers-0.22.2 tqdm-4.67.1 transformers-4.57.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/python/lib/python3.13/site-packages (4.57.6)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/python/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers) (2026.1.4)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from rank_bm25) (2.4.1)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: transformers in /opt/python/lib/python3.13/site-packages (4.57.6)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch in /opt/python/lib/python3.13/site-packages (2.9.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/python/lib/python3.13/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/python/lib/python3.13/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/python/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/python/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in /opt/python/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading spacy-3.8.11-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (32.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (254 kB)\n",
      "Downloading murmurhash-1.0.15-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (133 kB)\n",
      "Downloading preshed-3.0.12-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (835 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m835.9/835.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.10-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: wasabi, typing-inspection, spacy-loggers, spacy-legacy, smart-open, sentencepiece, pydantic-core, murmurhash, cymem, cloudpathlib, click, catalogue, blis, annotated-types, typer-slim, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [spacy]m21/22\u001b[0m [spacy]]c]lib]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 click-8.3.1 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 sentencepiece-0.2.1 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3\n",
      "Collecting fr-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-md\n",
      "Successfully installed fr-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_md')\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: rank-bm25 in /opt/python/lib/python3.13/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/python/lib/python3.13/site-packages (from rank-bm25) (2.4.1)\n",
      "Requirement already satisfied: sentence-transformers in /opt/python/lib/python3.13/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: tqdm in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/python/lib/python3.13/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/python/lib/python3.13/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/python/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /opt/python/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/python/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/python/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/python/lib/python3.13/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.4)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /opt/python/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/python/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [openpyxl]1/2\u001b[0m [openpyxl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Requirement already satisfied: sentencepiece in /opt/python/lib/python3.13/site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "# on sup un encoding: utf-8\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "!pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "!pip install spacy transformers sentencepiece torch\n",
    "!python -m spacy download fr_core_news_md\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install rank-bm25\n",
    "!pip install sentence-transformers\n",
    "\n",
    "!pip install openpyxl\n",
    "!pip install sentencepiece\n",
    "\n",
    "# RAPPEL : relancer le kernel si pb détection de packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb80a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"CUDA available :\", torch.cuda.is_available())\n",
    "# print(\"GPU name :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# import torch\n",
    "# print(torch.cuda.memory_allocated() / 1e9, \"GB GPU used\")\n",
    "\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.75it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:04<00:00,  9.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    # device: int = -1  # -1 CPU, 0 GPU\n",
    "    device: int = 0  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \"\"\"\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # calcul similarities restreint\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        sims[i, :] = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy()}\n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # print(df1.head())\n",
    "    # print(df2.head())\n",
    "\n",
    "    # TEMPS : 1min9s\n",
    "    # NLP + traduction (1min9s)\n",
    "    translator = TranslatorFR2EN(device=0)  # GPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "\n",
    "    # TEMPS : 24s\n",
    "    # Matching BM25 -> Embeddings -> Top5 \n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "\n",
    "    # # Sauvegarde\n",
    "    # match_wide.to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5.xlsx\", index=False)\n",
    "    # match_wide.attrs[\"out_long\"].to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_long.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98166e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df1 anglais\n",
    "# print(df1p.head()) \n",
    "\n",
    "# # df2 anglais\n",
    "# print(df2p.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab021d65",
   "metadata": {},
   "source": [
    "### 2bis) Version où on remplace la similarité cosinus par une pénalisation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b61439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.62it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:04<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul TF-IDF...\n",
      "Calcul similarités hybrides (embeddings + TF-IDF)...\n",
      "Calcul métriques d'incertitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8686/3628384705.py:316: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = spearmanr(embedding_sims_all[i], tfidf_sims_all[i])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    # device: int = -1  # -1 CPU, 0 GPU\n",
    "    device: int = 0  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  DIFFÉRENCE ENTRE LES DEUX APPROCHES DE SIMILARITÉ                            ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# 1) SIMILARITÉ COSINUS PURE (commentée ci-dessous) :\n",
    "#    - Mesure uniquement l'orientation des vecteurs embeddings\n",
    "#    - Ignore complètement les mots exacts utilisés\n",
    "#    - Exemple : \"cathéter veineux\" vs \"tube sanguin\" → score élevé (sémantique)\n",
    "#    - Problème : peut matcher des produits sémantiquement proches mais techniquement \n",
    "#      différents (ex: \"seringue 5ml\" vs \"seringue 10ml\")\n",
    "\n",
    "# 2) SIMILARITÉ HYBRIDE COSINUS + TF-IDF (implémentation actuelle) :\n",
    "#    - Combine sémantique (embeddings) + lexical (TF-IDF)\n",
    "#    - TF-IDF donne plus de poids aux termes rares/spécifiques\n",
    "#    - Exemple : \"cathéter ventriculaire\" → \"ventriculaire\" pèse plus lourd que \n",
    "#      \"cathéter\" (plus commun)\n",
    "#    - Avantage : détecte les correspondances exactes de termes techniques tout en \n",
    "#      gardant la compréhension sémantique\n",
    "#    - Paramètre alpha : contrôle l'équilibre entre les deux mesures\n",
    "#      * alpha=1.0 → 100% embeddings (cosinus pur)\n",
    "#      * alpha=0.0 → 100% TF-IDF (lexical pur)\n",
    "#      * alpha=0.6 → compromis pour nomenclatures médicales\n",
    "\n",
    "# INTERPRÉTABILITÉ :\n",
    "# - Score final = (0.6 × similarité_sémantique) + (0.4 × importance_termes_communs)\n",
    "# - Favorise les produits qui sont à la fois :\n",
    "#   1) Sémantiquement proches (compris par le modèle)\n",
    "#   2) Partageant des termes techniques spécifiques\n",
    "\n",
    "\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    "    alpha: float = 0.6,  # Pondération embeddings vs TF-IDF\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings + TF-IDF rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \n",
    "    Paramètres:\n",
    "        alpha: Poids des embeddings (1-alpha = poids TF-IDF)\n",
    "               alpha=1.0 → similarité cosinus pure\n",
    "               alpha=0.6 → recommandé (équilibre sémantique/lexical)\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.stats import spearmanr\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # TF-IDF sur corpus df1 unique\n",
    "    print(\"Calcul TF-IDF...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_matrix_df1 = vectorizer.fit_transform(df1u[df1_text_col].fillna(\"\"))\n",
    "    \n",
    "    # TF-IDF sur df2\n",
    "    tfidf_matrix_df2 = vectorizer.transform(df2[df2_text_col].fillna(\"\"))\n",
    "\n",
    "    # calcul similarities restreint (HYBRIDE)\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "    \n",
    "    # Stockage pour calcul d'incertitude\n",
    "    embedding_sims_all = np.empty((n2, topk_bm25), dtype=float)\n",
    "    tfidf_sims_all = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    print(\"Calcul similarités hybrides (embeddings + TF-IDF)...\")\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        \n",
    "        # # APPROCHE 1 (commentée) : Similarité cosinus pure sur embeddings\n",
    "        # cos_sim = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "        # sims[i, :] = cos_sim\n",
    "        \n",
    "        # APPROCHE 2 (actuelle) : Hybride embeddings + TF-IDF\n",
    "        # 1) Similarité sémantique (embeddings)\n",
    "        embedding_sim = emb2[i] @ emb1[idx].T  # (topk_bm25,)\n",
    "        \n",
    "        # 2) Similarité lexicale (TF-IDF sur termes communs)\n",
    "        tfidf_sim = (tfidf_matrix_df2[i] @ tfidf_matrix_df1[idx].T).toarray()[0]  # (topk_bm25,)\n",
    "        \n",
    "        # Stockage pour métriques d'incertitude\n",
    "        embedding_sims_all[i, :] = embedding_sim\n",
    "        tfidf_sims_all[i, :] = tfidf_sim\n",
    "        \n",
    "        # 3) Combinaison pondérée\n",
    "        sims[i, :] = alpha * embedding_sim + (1 - alpha) * tfidf_sim\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "    \n",
    "    # ============================================================\n",
    "    # QUANTIFICATION D'INCERTITUDE\n",
    "    # ============================================================\n",
    "    \n",
    "    # 1) Entropie normalisée de la distribution de probabilités\n",
    "    # Interprétation: >0.7 = très incertain (probs uniformes)\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
    "    normalized_entropy = entropy / np.log(topk_bm25)\n",
    "    \n",
    "    # 2) Variance des similarités brutes (avant softmax)\n",
    "    # Interprétation: variance élevée = scores bien différenciés (bon signal)\n",
    "    #                 variance faible = tous les candidats se ressemblent (mauvais signal)\n",
    "    sim_variance = np.var(sims, axis=1)\n",
    "    \n",
    "    # 3) Désaccord entre embeddings et TF-IDF (corrélation de Spearman)\n",
    "    # Interprétation: >0.5 = sémantique et lexique pointent vers candidats différents\n",
    "    print(\"Calcul métriques d'incertitude...\")\n",
    "    disagreement = np.zeros(n2)\n",
    "    for i in range(n2):\n",
    "        corr, _ = spearmanr(embedding_sims_all[i], tfidf_sims_all[i])\n",
    "        disagreement[i] = 1 - corr  # 0=accord parfait, 1=désaccord total\n",
    "    \n",
    "    # ============================================================\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\n",
    "        \"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy(),\n",
    "        \"uncertainty_entropy\": normalized_entropy,\n",
    "        \"similarity_variance\": sim_variance,\n",
    "        \"embedding_tfidf_disagreement\": disagreement,\n",
    "    }\n",
    "    \n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # print(df1.head())\n",
    "    # print(df2.head())\n",
    "\n",
    "    # TEMPS : 1min8s\n",
    "    # NLP + traduction (1min9s)\n",
    "    translator = TranslatorFR2EN(device=0)  # GPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "\n",
    "    # TEMPS : 9.7s\n",
    "    # Matching BM25 -> Embeddings+TF-IDF -> Top5 \n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07,\n",
    "        alpha=0.70  # 70% similarité cos, 30% TF-IDF\n",
    "        # alpha=0  # 70% similarité cos, 30% TF-IDF\n",
    "    )\n",
    "\n",
    "    # # Sauvegarde\n",
    "    # match_wide.to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_simcos_et_TF_IDF.xlsx\", index=False)\n",
    "    # match_wide.attrs[\"out_long\"].to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_long_simcos_et_TF_IDF.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2aed4",
   "metadata": {},
   "source": [
    "#### Jointure finale entre les deux tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d13b8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pred = pd.read_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_simcos_et_TF_IDF.xlsx\")\n",
    "df_emission = pd.read_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\")\n",
    "\n",
    "\n",
    "df = df_pred.copy()\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df = df.merge(\n",
    "        df_emission[[\"produit\", \"Emission_kgCO2e_unitaire\"]],\n",
    "        how=\"left\",\n",
    "        left_on=f\"top{i}_produit\",\n",
    "        right_on=\"produit\"\n",
    "    ).rename(columns={\n",
    "        \"Emission_kgCO2e_unitaire\": f\"emission_top{i}\"\n",
    "    }).drop(columns=[\"produit\"])\n",
    "\n",
    "df[\"sum_proba\"] = sum(df[f\"top{i}_proba\"] for i in range(1, 6))\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df[f\"top{i}_proba_norm\"] = df[f\"top{i}_proba\"] / df[\"sum_proba\"]\n",
    "\n",
    "df[\"Emission_carbone_attendue_kgCO2e\"] = sum(\n",
    "    df[f\"top{i}_proba_norm\"] * df[f\"emission_top{i}\"]\n",
    "    for i in range(1, 6)\n",
    ")\n",
    "\n",
    "df.to_excel(\"Axe2_final_predictions_avec_emission_carbone.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e87b6",
   "metadata": {},
   "source": [
    "### 3) Réduction et évaluation des coûts carbones \n",
    "\n",
    "Le site de [EcoLogits](https://ecologits.ai/latest/reference/tracers/utils/#tracers.utils.llm_impacts) peut être utile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d582a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLM, TintBERT ?\n",
    "# Métriques et graphes de consommation carbonne par inférence\n",
    "\n",
    "\n",
    "# INTÉGRATION ECOLOGITS + CODECARBON\n",
    "# ===================================\n",
    "\n",
    "# Combine les deux outils pour une analyse complète :\n",
    "# - CodeCarbon : mesures générales (spaCy, TF-IDF, BM25)\n",
    "# - EcoLogits : métriques détaillées pour LLMs (traduction, embeddings)\n",
    "\n",
    "# Installation : pip install ecologits codecarbon\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "\n",
    "# !pip install codecarbon\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# !pip install ecologits\n",
    "from ecologits.tracers.utils import llm_impacts\n",
    "ECOLOGITS_AVAILABLE = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b23bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:17:57] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:17:57] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:17:57] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure spaCy français...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:18:02] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:18:02] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:18:02] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure spaCy anglais...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:18:07] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:18:07] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:18:07] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure traduction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n",
      "[codecarbon WARNING @ 06:18:12] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:18:12] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:18:12] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 06:18:19] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon WARNING @ 06:18:19] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 06:18:19] No CPU tracking mode found. Falling back on CPU load mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Mesure TF-IDF + BM25...\n",
      "\n",
      "====================================================================================================\n",
      "📊 RAPPORT CARBONE HYBRIDE - Projet: medical_nomenclature_matching\n",
      "====================================================================================================\n",
      "\n",
      "🔬 Modèles classiques: 3\n",
      "🤖 Modèles LLM: 2\n",
      "\n",
      "⚡ Énergie totale (CodeCarbon): 0.000289 kWh\n",
      "🏭 CO2 total (CodeCarbon): 0.000016 kg\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Détail par modèle:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                Modèle    Type  Durée (s)  Énergie (kWh)  CO2 CodeCarbon (kg)\n",
      " spaCy_fr_core_news_md classic       5.74       0.000073             0.000004\n",
      "  spaCy_en_core_web_sm classic       4.73       0.000036             0.000002\n",
      "Helsinki_opus-mt-fr-en     llm       5.22       0.000055             0.000003\n",
      " S-PubMedBert-MS-MARCO     llm       6.61       0.000106             0.000006\n",
      " TF-IDF_BM25_Reranking classic       4.32       0.000019             0.000001\n",
      "\n",
      "====================================================================================================\n",
      "🌳 ÉQUIVALENCES CARBONE\n",
      "====================================================================================================\n",
      "• 0.0 km en voiture\n",
      "• 0.00 arbres pendant 1 an pour compenser\n",
      "• 24 charges de smartphone\n",
      "✅ Résultats sauvegardés : /home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/carbon_footprint_hybrid_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INTÉGRATION ECOLOGITS + CODECARBON\n",
    "===================================\n",
    "\n",
    "Combine les deux outils pour une analyse complète :\n",
    "- CodeCarbon : mesures générales (spaCy, TF-IDF, BM25)\n",
    "- EcoLogits : métriques détaillées pour LLMs (traduction, embeddings)\n",
    "\n",
    "Installation : pip install ecologits codecarbon\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import time\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "try:\n",
    "    from ecologits.tracers.utils import llm_impacts\n",
    "    ECOLOGITS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ECOLOGITS_AVAILABLE = False\n",
    "    print(\"⚠️ EcoLogits non disponible. Installez avec : pip install ecologits\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhancedModelMetrics:\n",
    "    \"\"\"Métriques enrichies combinant CodeCarbon + EcoLogits\"\"\"\n",
    "    model_name: str\n",
    "    model_type: str  # \"llm\" ou \"classic\"\n",
    "    \n",
    "    # Métriques communes\n",
    "    duration_s: float\n",
    "    energy_kwh: float\n",
    "    \n",
    "    # Métriques CodeCarbon (tous modèles)\n",
    "    co2_kg: float\n",
    "    cpu_power_w: float = 0.0\n",
    "    gpu_power_w: float = 0.0\n",
    "    ram_power_w: float = 0.0\n",
    "    \n",
    "    # Métriques EcoLogits (LLMs uniquement)\n",
    "    gwp_kg_co2eq: Optional[float] = None      # Global Warming Potential\n",
    "    adpe_kg_sb_eq: Optional[float] = None     # Abiotic Depletion (métaux)\n",
    "    pe_mj: Optional[float] = None             # Primary Energy\n",
    "    wcf_liters: Optional[float] = None        # Water Consumption\n",
    "    usage_gwp: Optional[float] = None         # GWP usage seulement\n",
    "    embodied_gwp: Optional[float] = None      # GWP embodied seulement\n",
    "    \n",
    "    # Métadonnées LLM\n",
    "    tokens_processed: Optional[int] = None\n",
    "    latency_per_token_ms: Optional[float] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        base = {\n",
    "            \"Modèle\": self.model_name,\n",
    "            \"Type\": self.model_type,\n",
    "            \"Durée (s)\": round(self.duration_s, 2),\n",
    "            \"Énergie (kWh)\": round(self.energy_kwh, 6),\n",
    "            \"CO2 CodeCarbon (kg)\": round(self.co2_kg, 6),\n",
    "        }\n",
    "        \n",
    "        # Ajout métriques EcoLogits si disponibles\n",
    "        if self.model_type == \"llm\" and self.gwp_kg_co2eq is not None:\n",
    "            base.update({\n",
    "                \"GWP total (kg CO2eq)\": round(self.gwp_kg_co2eq, 6),\n",
    "                \"GWP usage (kg)\": round(self.usage_gwp or 0, 6),\n",
    "                \"GWP embodied (kg)\": round(self.embodied_gwp or 0, 6),\n",
    "                \"ADPe (kg Sb eq)\": round(self.adpe_kg_sb_eq or 0, 9),\n",
    "                \"Énergie primaire (MJ)\": round(self.pe_mj or 0, 3),\n",
    "                \"Eau (litres)\": round(self.wcf_liters or 0, 3),\n",
    "                \"Tokens traités\": self.tokens_processed or 0,\n",
    "                \"Latence/token (ms)\": round(self.latency_per_token_ms or 0, 2),\n",
    "            })\n",
    "        \n",
    "        return base\n",
    "\n",
    "\n",
    "class HybridCarbonBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark hybride utilisant CodeCarbon ET EcoLogits\n",
    "    \n",
    "    Usage:\n",
    "        bench = HybridCarbonBenchmark()\n",
    "        \n",
    "        # Modèle classique (spaCy)\n",
    "        with bench.track_classic(\"spacy_fr\"):\n",
    "            nlp = spacy.load(\"fr_core_news_md\")\n",
    "            docs = list(nlp.pipe(texts))\n",
    "        \n",
    "        # Modèle LLM (traduction)\n",
    "        with bench.track_llm(\"translation\", provider=\"huggingface\", model=\"Helsinki-NLP/opus-mt-fr-en\"):\n",
    "            outputs = translator(texts)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, country_code: str = \"FRA\", project_name: str = \"medical_matching\"):\n",
    "        self.country_code = country_code\n",
    "        self.project_name = project_name\n",
    "        self.results: List[EnhancedModelMetrics] = []\n",
    "        \n",
    "        if not ECOLOGITS_AVAILABLE:\n",
    "            print(\"⚠️ EcoLogits non disponible - métriques limitées à CodeCarbon\")\n",
    "    \n",
    "    def track_classic(self, model_name: str):\n",
    "        \"\"\"Track un modèle classique (spaCy, sklearn, etc.) avec CodeCarbon\"\"\"\n",
    "        return _ClassicModelTracker(self, model_name)\n",
    "    \n",
    "    def track_llm(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        provider: str = \"huggingface\",\n",
    "        model_id: str = None,\n",
    "        electricity_mix_zone: str = None\n",
    "    ):\n",
    "        \"\"\"Track un LLM avec CodeCarbon + EcoLogits\"\"\"\n",
    "        return _LLMTracker(self, model_name, provider, model_id, electricity_mix_zone)\n",
    "    \n",
    "    def add_result(self, metrics: EnhancedModelMetrics):\n",
    "        self.results.append(metrics)\n",
    "    \n",
    "    def get_dataframe(self) -> pd.DataFrame:\n",
    "        if not self.results:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame([r.to_dict() for r in self.results])\n",
    "    \n",
    "    def print_summary(self):\n",
    "        if not self.results:\n",
    "            print(\"Aucune mesure disponible\")\n",
    "            return\n",
    "        \n",
    "        df = self.get_dataframe()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(f\"📊 RAPPORT CARBONE HYBRIDE - Projet: {self.project_name}\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Séparation LLM vs Classic\n",
    "        llm_results = [r for r in self.results if r.model_type == \"llm\"]\n",
    "        classic_results = [r for r in self.results if r.model_type == \"classic\"]\n",
    "        \n",
    "        print(f\"\\n🔬 Modèles classiques: {len(classic_results)}\")\n",
    "        print(f\"🤖 Modèles LLM: {len(llm_results)}\")\n",
    "        \n",
    "        # Total CodeCarbon\n",
    "        total_co2_cc = sum(r.co2_kg for r in self.results)\n",
    "        total_energy = sum(r.energy_kwh for r in self.results)\n",
    "        \n",
    "        print(f\"\\n⚡ Énergie totale (CodeCarbon): {total_energy:.6f} kWh\")\n",
    "        print(f\"🏭 CO2 total (CodeCarbon): {total_co2_cc:.6f} kg\")\n",
    "        \n",
    "        # Si EcoLogits disponible, afficher métriques enrichies\n",
    "        if llm_results and llm_results[0].gwp_kg_co2eq is not None:\n",
    "            total_gwp = sum(r.gwp_kg_co2eq or 0 for r in llm_results)\n",
    "            total_adpe = sum(r.adpe_kg_sb_eq or 0 for r in llm_results)\n",
    "            total_pe = sum(r.pe_mj or 0 for r in llm_results)\n",
    "            total_water = sum(r.wcf_liters or 0 for r in llm_results)\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*100)\n",
    "            print(\"🌍 MÉTRIQUES ENRICHIES ECOLOGITS (LLMs uniquement)\")\n",
    "            print(\"-\"*100)\n",
    "            print(f\"• GWP total: {total_gwp:.6f} kg CO2eq\")\n",
    "            print(f\"  ├─ Usage: {sum(r.usage_gwp or 0 for r in llm_results):.6f} kg\")\n",
    "            print(f\"  └─ Embodied: {sum(r.embodied_gwp or 0 for r in llm_results):.6f} kg\")\n",
    "            print(f\"• ADPe (épuisement métaux): {total_adpe:.9f} kg Sb eq\")\n",
    "            print(f\"• Énergie primaire: {total_pe:.3f} MJ\")\n",
    "            print(f\"• Consommation d'eau: {total_water:.3f} litres\")\n",
    "            \n",
    "            # Équivalences eau\n",
    "            print(f\"\\n💧 Équivalent eau:\")\n",
    "            print(f\"  • {total_water / 0.25:.0f} verres d'eau (250ml)\")\n",
    "            print(f\"  • {total_water / 8:.1f} douches (8L/min pendant 1min)\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*100)\n",
    "        print(\"Détail par modèle:\")\n",
    "        print(\"-\"*100)\n",
    "        print(df.to_string(index=False))\n",
    "        \n",
    "        # Équivalences carbone\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"🌳 ÉQUIVALENCES CARBONE\")\n",
    "        print(\"=\"*100)\n",
    "        km_voiture = total_co2_cc / 0.12\n",
    "        arbres_an = total_co2_cc / 21\n",
    "        smartphones = total_energy * 1000 / 0.012\n",
    "        \n",
    "        print(f\"• {km_voiture:.1f} km en voiture\")\n",
    "        print(f\"• {arbres_an:.2f} arbres pendant 1 an pour compenser\")\n",
    "        print(f\"• {smartphones:.0f} charges de smartphone\")\n",
    "    \n",
    "    def save_results(self, filepath: str):\n",
    "        df = self.get_dataframe()\n",
    "        if df.empty:\n",
    "            print(\"Aucun résultat à sauvegarder\")\n",
    "            return\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name=\"Métriques complètes\", index=False)\n",
    "            \n",
    "            # Onglet comparaison LLM vs Classic\n",
    "            comparison = pd.DataFrame({\n",
    "                \"Type\": [\"Modèles classiques\", \"Modèles LLM\", \"TOTAL\"],\n",
    "                \"Nombre\": [\n",
    "                    len([r for r in self.results if r.model_type == \"classic\"]),\n",
    "                    len([r for r in self.results if r.model_type == \"llm\"]),\n",
    "                    len(self.results)\n",
    "                ],\n",
    "                \"CO2 (kg)\": [\n",
    "                    sum(r.co2_kg for r in self.results if r.model_type == \"classic\"),\n",
    "                    sum(r.co2_kg for r in self.results if r.model_type == \"llm\"),\n",
    "                    sum(r.co2_kg for r in self.results)\n",
    "                ],\n",
    "                \"Énergie (kWh)\": [\n",
    "                    sum(r.energy_kwh for r in self.results if r.model_type == \"classic\"),\n",
    "                    sum(r.energy_kwh for r in self.results if r.model_type == \"llm\"),\n",
    "                    sum(r.energy_kwh for r in self.results)\n",
    "                ]\n",
    "            })\n",
    "            comparison.to_excel(writer, sheet_name=\"LLM vs Classic\", index=False)\n",
    "        \n",
    "        print(f\"✅ Résultats sauvegardés : {filepath}\")\n",
    "\n",
    "\n",
    "class _ClassicModelTracker:\n",
    "    \"\"\"Tracker pour modèles classiques (CodeCarbon uniquement)\"\"\"\n",
    "    \n",
    "    def __init__(self, benchmark: HybridCarbonBenchmark, model_name: str):\n",
    "        self.benchmark = benchmark\n",
    "        self.model_name = model_name\n",
    "        self.tracker = None\n",
    "        self.start_time = None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        # CodeCarbon 3.x utilise country_2letter_iso_code (pas country_iso_code)\n",
    "        country_code_2letter = self.benchmark.country_code[:2] if len(self.benchmark.country_code) > 2 else self.benchmark.country_code\n",
    "        self.tracker = EmissionsTracker(\n",
    "            project_name=self.benchmark.project_name,\n",
    "            # country_2letter_iso_code=country_code_2letter,\n",
    "            output_dir=\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results\",\n",
    "            log_level=\"warning\",\n",
    "        )\n",
    "        self.tracker.start()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        emissions = self.tracker.stop()\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        metrics = EnhancedModelMetrics(\n",
    "            model_name=self.model_name,\n",
    "            model_type=\"classic\",\n",
    "            duration_s=duration,\n",
    "            energy_kwh=self.tracker._total_energy.kWh if hasattr(self.tracker, '_total_energy') else 0,\n",
    "            co2_kg=emissions if emissions else 0,\n",
    "            cpu_power_w=self.tracker._cpu_power.W if hasattr(self.tracker, '_cpu_power') else 0,\n",
    "            gpu_power_w=self.tracker._gpu_power.W if hasattr(self.tracker, '_gpu_power') else 0,\n",
    "            ram_power_w=self.tracker._ram_power.W if hasattr(self.tracker, '_ram_power') else 0,\n",
    "        )\n",
    "        \n",
    "        self.benchmark.add_result(metrics)\n",
    "\n",
    "\n",
    "class _LLMTracker:\n",
    "    \"\"\"Tracker pour LLMs (CodeCarbon + EcoLogits)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        benchmark: HybridCarbonBenchmark, \n",
    "        model_name: str,\n",
    "        provider: str,\n",
    "        model_id: str,\n",
    "        electricity_mix_zone: str\n",
    "    ):\n",
    "        self.benchmark = benchmark\n",
    "        self.model_name = model_name\n",
    "        self.provider = provider\n",
    "        self.model_id = model_id or model_name\n",
    "        self.electricity_mix_zone = electricity_mix_zone or benchmark.country_code\n",
    "        self.tracker = None\n",
    "        self.start_time = None\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.tracker = EmissionsTracker(\n",
    "            project_name=self.benchmark.project_name,\n",
    "            # country_iso_code=self.benchmark.country_code,\n",
    "            output_dir=\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results\",\n",
    "            log_level=\"WARNING\",\n",
    "        )\n",
    "        self.tracker.start()\n",
    "        return self\n",
    "    \n",
    "    def set_token_count(self, count: int):\n",
    "        \"\"\"Permet de définir le nombre de tokens traités\"\"\"\n",
    "        self.token_count = count\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        emissions_cc = self.tracker.stop()\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        # Métriques CodeCarbon\n",
    "        energy_kwh = self.tracker._total_energy.kWh if hasattr(self.tracker, '_total_energy') else 0\n",
    "        \n",
    "        # Métriques EcoLogits (si disponible)\n",
    "        gwp = adpe = pe = wcf = usage_gwp = embodied_gwp = None\n",
    "        latency_per_token = None\n",
    "        \n",
    "        if ECOLOGITS_AVAILABLE and self.token_count > 0:\n",
    "            try:\n",
    "                impacts = llm_impacts(\n",
    "                    provider=self.provider,\n",
    "                    model_name=self.model_id,\n",
    "                    output_token_count=self.token_count,\n",
    "                    request_latency=duration,\n",
    "                    electricity_mix_zone=self.electricity_mix_zone,\n",
    "                )\n",
    "                \n",
    "                if impacts.gwp:\n",
    "                    gwp = impacts.gwp.value\n",
    "                    usage_gwp = impacts.usage.gwp.value if impacts.usage and impacts.usage.gwp else None\n",
    "                    embodied_gwp = impacts.embodied.gwp.value if impacts.embodied and impacts.embodied.gwp else None\n",
    "                \n",
    "                adpe = impacts.adpe.value if impacts.adpe else None\n",
    "                pe = impacts.pe.value if impacts.pe else None\n",
    "                wcf = impacts.wcf.value if impacts.wcf else None\n",
    "                latency_per_token = (duration / self.token_count) * 1000  # ms\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur EcoLogits pour {self.model_name}: {e}\")\n",
    "        \n",
    "        metrics = EnhancedModelMetrics(\n",
    "            model_name=self.model_name,\n",
    "            model_type=\"llm\",\n",
    "            duration_s=duration,\n",
    "            energy_kwh=energy_kwh,\n",
    "            co2_kg=emissions_cc if emissions_cc else 0,\n",
    "            gwp_kg_co2eq=gwp,\n",
    "            adpe_kg_sb_eq=adpe,\n",
    "            pe_mj=pe,\n",
    "            wcf_liters=wcf,\n",
    "            usage_gwp=usage_gwp,\n",
    "            embodied_gwp=embodied_gwp,\n",
    "            tokens_processed=self.token_count if self.token_count > 0 else None,\n",
    "            latency_per_token_ms=latency_per_token,\n",
    "        )\n",
    "        \n",
    "        self.benchmark.add_result(metrics)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXEMPLE D'UTILISATION AVEC TON PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Exemple d'intégration complète dans ton pipeline de matching\n",
    "    \"\"\"\n",
    "\n",
    "    # TOTAL : 26s\n",
    "    \n",
    "    bench = HybridCarbonBenchmark(\n",
    "        country_code=\"FRA\",\n",
    "        project_name=\"medical_nomenclature_matching\"\n",
    "    )\n",
    "    \n",
    "    # 1) spaCy (classique)\n",
    "    print(\"📊 Mesure spaCy français...\")\n",
    "    with bench.track_classic(\"spaCy_fr_core_news_md\"):\n",
    "        import spacy\n",
    "        nlp_fr = spacy.load(\"fr_core_news_md\", disable=[\"ner\", \"parser\"])\n",
    "        # texts_fr = [\"exemple\"] * 1000\n",
    "        # docs = list(nlp_fr.pipe(texts_fr, batch_size=256))\n",
    "    \n",
    "    # 2) spaCy anglais (classique)\n",
    "    print(\"📊 Mesure spaCy anglais...\")\n",
    "    with bench.track_classic(\"spaCy_en_core_web_sm\"):\n",
    "        nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "        # docs = list(nlp_en.pipe(texts_en, batch_size=256))\n",
    "    \n",
    "    # 3) Traduction (LLM)\n",
    "    print(\"📊 Mesure traduction...\")\n",
    "    tracker_translation = bench.track_llm(\n",
    "        model_name=\"Helsinki_opus-mt-fr-en\",\n",
    "        provider=\"huggingface\",\n",
    "        model_id=\"Helsinki-NLP/opus-mt-fr-en\",\n",
    "        electricity_mix_zone=\"FRA\"\n",
    "    )\n",
    "    \n",
    "    with tracker_translation:\n",
    "        from transformers import pipeline\n",
    "        translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\", device=0)\n",
    "        # texts_fr = [\"exemple\"] * 1200\n",
    "        # outputs = translator(texts_fr, batch_size=16, truncation=True)\n",
    "        \n",
    "        # Compter les tokens générés (approximation)\n",
    "        # token_count = sum(len(o[\"translation_text\"].split()) for o in outputs)\n",
    "        token_count = 1200 * 15  # Approximation : 15 tokens/texte\n",
    "        tracker_translation.set_token_count(token_count)\n",
    "    \n",
    "    # 4) Embeddings (LLM)\n",
    "    print(\"📊 Mesure embeddings...\")\n",
    "    tracker_embeddings = bench.track_llm(\n",
    "        model_name=\"S-PubMedBert-MS-MARCO\",\n",
    "        provider=\"huggingface\",\n",
    "        model_id=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        electricity_mix_zone=\"FRA\"\n",
    "    )\n",
    "    \n",
    "    with tracker_embeddings:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model_emb = SentenceTransformer(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "        # texts = [\"exemple\"] * 6200\n",
    "        # embeddings = model_emb.encode(texts, normalize_embeddings=True, batch_size=64)\n",
    "        \n",
    "        # Approximation tokens (BERT = ~1.3 token/mot)\n",
    "        token_count = 6200 * 10 * 1.3  # 6200 textes × 10 mots × 1.3\n",
    "        tracker_embeddings.set_token_count(int(token_count))\n",
    "    \n",
    "    # 5) TF-IDF + BM25 (classique)\n",
    "    print(\"📊 Mesure TF-IDF + BM25...\")\n",
    "    with bench.track_classic(\"TF-IDF_BM25_Reranking\"):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        # vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        # tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Ton code BM25 + reranking\n",
    "        pass\n",
    "    \n",
    "    # Résultats\n",
    "    bench.print_summary()\n",
    "    bench.save_results(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/carbon_footprint_hybrid_report.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "317ec77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "import codecarbon\n",
    "print(codecarbon.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
