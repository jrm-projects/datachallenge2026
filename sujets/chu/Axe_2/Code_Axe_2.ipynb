{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d75ee",
   "metadata": {},
   "source": [
    "# Axe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1beb3",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4437d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7153e13",
   "metadata": {},
   "source": [
    "### 1) Processing du df composant médical - Emission carbonne (kgCO2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392c977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees\n",
      "0    pansements composites                    0.9400              m2\n",
      "1           Sonde urinaire                   51.9034      parProduit\n",
      "2  Set de sondage urinaire                   89.4064      parProduit\n",
      "3      Collecteur de jambe                  250.7186      parProduit\n",
      "4       Collecteur de nuit                  676.6522      parProduit\n",
      "{'pansements composites': 2880.7506438973483, 'Sonde urinaire': 2.3058391190603773, 'Set de sondage urinaire': 1301.0278534529064, 'Collecteur de jambe': 1871.2095479103848, 'Collecteur de nuit': 2265.768898794339, 'Etuis péniens': 1088.7167868279912, 'Poche pour stomie': 3.131259971503482, 'Support pour stomie': 2959.0238083770532, 'Changes complets': 2375.889528275952, 'Slips absorbants': 250.72891706628525, 'Protections absorbantes': 1887.063988630245, 'Couches droites': 1329.456550312355, 'Alèses': 1383.9846297425656, 'pansement': 2495.056711895948, 'uteruscopes': 2579.2114069150794, 'instrument usage unique': 1233.5956502107485, 'bronchoscope': 2584.057077104993, 'verre lunettes': 570.6534648859387, 'Scanners': 1494.6674379613792, 'IRM (dont cage de Faraday. hélium)': 1479.9295216240887, 'Caméras à scintillation': 399.9716995350735, 'Tomographes à émission/ caméras à positons': 1949.3421425722856, 'Salles de radiologie': 2073.3922006142684, 'Appareils de mammographie': 568.0886777818828, 'Appareils de radiographie dentaire standard': 1571.6806541816616, 'Appareils de radiographie dentaire panoramique': 1792.2833323467103, 'Echographes portables': 2530.639912909539, 'Echographes fixes': 2426.4950561912756, 'Arceaux mobiles de radioscopie': 2592.063547762132, 'Statifs vasculaires avec arceau': 663.5845948900616, 'DAE': 1897.2515853423176, \"Générateurs d'hémodialyse\": 983.7099696401053, 'Equipements de radiothérapie': 1520.2436884291217, \"Ventilateurs d'anesthésie et de réanimation\": 197.9303483872975, 'Robots chirurgicaux': 1319.1239098347266, 'Pompes à perfusion': 596.6801935342754, 'Pousse-seringues': 1453.2358761107366, 'Pompes à nutrition (entérale ou parentérale)': 2705.902467790883, 'Autoclaves': 605.179419391841, 'Laveurs désinfecteurs': 2099.402660695002, 'Colonnes d’endoscopie et de coelioscopie': 1135.8599146287866, 'Moniteurs multi-paramétriques (ECG/PNI/SPO2. etc.)': 1912.330303059016, 'Aspirateurs de mucosités': 935.9956224870981, 'complement alimentaire': 1076.3137946700936}\n",
      "                   produit  Emission_kgCO2e_unitaire type_de_donnees  \\\n",
      "0    pansements composites                    0.9400              m2   \n",
      "1           Sonde urinaire                   51.9034      parProduit   \n",
      "2  Set de sondage urinaire                   89.4064      parProduit   \n",
      "3      Collecteur de jambe                  250.7186      parProduit   \n",
      "4       Collecteur de nuit                  676.6522      parProduit   \n",
      "\n",
      "   Emission_carbonne_total_des_produits_kgCO2e  \n",
      "0                                 2.820000e+03  \n",
      "1                                 1.196809e+02  \n",
      "2                                 1.163202e+05  \n",
      "3                                 4.691470e+05  \n",
      "4                                 1.533138e+06  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def concat_xlsx_from_folder(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parcourt récursivement un dossier git et concatène verticalement\n",
    "    toutes les tables issues des fichiers .xlsx.\n",
    "\n",
    "    Hypothèses :\n",
    "    - chaque fichier .xlsx contient une table avec exactement 2 colonnes\n",
    "    - la première ligne correspond aux labels et est ignorée\n",
    "    - colonne A : produit\n",
    "    - colonne B : Emission_kgCO2e_unitaire\n",
    "    \"\"\"\n",
    "\n",
    "    folder = Path(folder_path)\n",
    "    all_rows = []\n",
    "\n",
    "    for file in folder.rglob(\"*.xlsx\"):\n",
    "        df = pd.read_excel(file, header=0)\n",
    "\n",
    "        df = df.iloc[:, :2]\n",
    "        df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "        filename = file.stem\n",
    "        if filename.endswith(\"_parProduit\"):\n",
    "            df[\"type_de_donnees\"] = \"parProduit\"\n",
    "        elif filename.endswith(\"_m2\"):\n",
    "            df[\"type_de_donnees\"] = \"m2\"\n",
    "        elif filename.endswith(\"_parKG\"):\n",
    "            df[\"type_de_donnees\"] = \"parKG\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_xlsx_from_folders(list_paths: list[str | Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatène verticalement les tables issues de plusieurs dossiers.\n",
    "\n",
    "    Paramètre\n",
    "    ----------\n",
    "    list_paths : list[str | Path]\n",
    "        Liste de chemins vers des dossiers contenant des fichiers .xlsx\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    DataFrame avec les colonnes :\n",
    "    - produit\n",
    "    - Emission_kgCO2e_unitaire\n",
    "    - type_de_donnees\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for folder_path in list_paths:\n",
    "        folder = Path(folder_path)\n",
    "\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Dossier introuvable : {folder}\")\n",
    "\n",
    "        xlsx_files = list(folder.rglob(\"*.xlsx\"))\n",
    "        if not xlsx_files:\n",
    "            raise ValueError(f\"Aucun fichier .xlsx trouvé dans {folder}\")\n",
    "\n",
    "        for file in xlsx_files:\n",
    "            df = pd.read_excel(file)\n",
    "\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"produit\", \"Emission_kgCO2e_unitaire\"]\n",
    "\n",
    "            name = file.stem\n",
    "            if name.endswith(\"_parProduit\"):\n",
    "                df[\"type_de_donnees\"] = \"parProduit\"\n",
    "            elif name.endswith(\"_m2\"):\n",
    "                df[\"type_de_donnees\"] = \"m2\"\n",
    "            elif name.endswith(\"_parKG\"):\n",
    "                df[\"type_de_donnees\"] = \"parKG\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"produit\", \"Emission_kgCO2e_unitaire\", \"type_de_donnees\"]\n",
    "        )\n",
    "\n",
    "    df = pd.concat(all_rows, axis=0, ignore_index=True)\n",
    "    \n",
    "    df[\"Emission_kgCO2e_unitaire\"] = (\n",
    "        df[\"Emission_kgCO2e_unitaire\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "\n",
    "    df[\"Emission_kgCO2e_unitaire\"] = pd.to_numeric(\n",
    "        df[\"Emission_kgCO2e_unitaire\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def random_value_dict(\n",
    "    df: pd.DataFrame,\n",
    "    nom_col: str = \"produit\",\n",
    "    nom_to_ignore: list | None = None,\n",
    "    min_value: float = 1.0,\n",
    "    max_value: float = 3000.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Construit un dictionnaire :\n",
    "    - clés : valeurs uniques de df[nom_col]\n",
    "    - valeurs : nombre aléatoire strictement > 1\n",
    "      tiré dans [min_value, max_value]\n",
    "    \"\"\"\n",
    "\n",
    "    if nom_to_ignore is None:\n",
    "        nom_to_ignore = []\n",
    "\n",
    "    if min_value <= 1:\n",
    "        min_value = 1.000001\n",
    "\n",
    "    uniques = df[nom_col].dropna().unique()\n",
    "\n",
    "    return {\n",
    "        val: random.uniform(min_value, max_value)\n",
    "        for val in uniques\n",
    "        if val not in nom_to_ignore\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_emission_hopital(\n",
    "    df: pd.DataFrame,\n",
    "    type_de_donnees: str = \"type_de_donnees\",\n",
    "    dict_m2: dict = {\"pansements composites\": (3 * 10**2, 10)},\n",
    "    dict_parKG: dict = {\"instrument usage unique\": 1000, \"complement alimentaire\":10000},\n",
    "    dict_nb_parProduit: dict | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ajoute la colonne Emission_carbonne_total_des_produits_kgCO2e selon\n",
    "    le type de données associé à chaque produit.\n",
    "    \"\"\"\n",
    "\n",
    "    if dict_nb_parProduit is None:\n",
    "        dict_nb_parProduit = {}\n",
    "\n",
    "    def compute_row(row):\n",
    "        produit = row[\"produit\"]\n",
    "        emission_unit = row[\"Emission_kgCO2e_unitaire\"]\n",
    "        t = row[type_de_donnees]\n",
    "\n",
    "        if t == \"parProduit\":\n",
    "            return emission_unit * dict_nb_parProduit.get(produit, np.nan)\n",
    "\n",
    "        if t == \"m2\":\n",
    "            longueur, largeur = dict_m2.get(produit, (np.nan, np.nan))\n",
    "            return emission_unit * longueur * largeur\n",
    "\n",
    "        if t == \"parKG\":\n",
    "            return emission_unit * dict_parKG.get(produit, np.nan)\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Emission_carbonne_total_des_produits_kgCO2e\"] = df.apply(compute_row, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# =========================\n",
    "# 3. Construction du DataFrame\n",
    "# =========================\n",
    "\n",
    "# extract_path = \"sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# extract_path = \"C:/Users/jerem/Documents/GitHub/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "extract_path = \"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/Axe_2_bdd\"\n",
    "# paths = [\n",
    "#     r\"sujets\\chu\\Axe_2\\Axe_2_bdd-20260117T004817Z-1-001\\Axe_2_bdd\",\n",
    "#     r\"sujets\\chu\\Axe_2\\autre_dossier\"\n",
    "# ]\n",
    "\n",
    "df_concat = concat_xlsx_from_folder(extract_path)\n",
    "# df_concat = concat_xlsx_from_folders(paths)\n",
    "\n",
    "print(df_concat.head())\n",
    "\n",
    "# =========================\n",
    "# 4. Dictionnaires exemples\n",
    "# =========================\n",
    "\n",
    "dict_nb_parProduit = random_value_dict(df_concat)\n",
    "\n",
    "print(dict_nb_parProduit)\n",
    "\n",
    "# =========================\n",
    "# 5. Calcul des émissions\n",
    "# =========================\n",
    "\n",
    "df_final = compute_emission_hopital(\n",
    "    df_concat,\n",
    "    dict_nb_parProduit=dict_nb_parProduit\n",
    ")\n",
    "\n",
    "print(df_final.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7ee391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons sur la colonne 'produit' :\n",
      "Empty DataFrame\n",
      "Columns: [produit, Emission_kgCO2e_unitaire, type_de_donnees, Emission_carbonne_total_des_produits_kgCO2e]\n",
      "Index: []\n",
      "\n",
      "Valeurs répétées dans 'produit' :\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Doublons sur une variable particulière (ex: 'email')\n",
    "colonne = 'produit'\n",
    "\n",
    "# Trouver les valeurs dupliquées dans cette colonne\n",
    "valeurs_doublons = df_final[df_final.duplicated(subset=[colonne], keep=False)]\n",
    "\n",
    "# Afficher les doublons triés pour mieux voir\n",
    "doublons_tries = valeurs_doublons.sort_values(colonne)\n",
    "print(f\"Doublons sur la colonne '{colonne}' :\")\n",
    "print(doublons_tries)\n",
    "\n",
    "# Voir les valeurs qui se répètent\n",
    "comptage = df_final[colonne].value_counts()\n",
    "valeurs_repetees = comptage[comptage > 1]\n",
    "print(f\"\\nValeurs répétées dans '{colonne}' :\")\n",
    "print(valeurs_repetees)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f786ffa",
   "metadata": {},
   "source": [
    "#### 1.1) Obtention du dataframe final et exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbd6cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_conserver = [\n",
    "    \"produit\",\n",
    "    \"Emission_kgCO2e_unitaire\",\n",
    "    \"Emission_carbonne_total_des_produits_kgCO2e\"\n",
    "]\n",
    "\n",
    "df_export = df_final[colonnes_a_conserver].copy()\n",
    "\n",
    "# =========================\n",
    "# Chemin de sortie\n",
    "# =========================\n",
    "\n",
    "output_path = Path(r\"results\\df_composant_medical_emissions_carbones.xlsx\")\n",
    "\n",
    "# Création du dossier si besoin\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Export Excel\n",
    "# =========================\n",
    "\n",
    "df_export.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a2ec7",
   "metadata": {},
   "source": [
    "### 2) Pre processing NLP des bases **df_composant_medical_emissions_carbones.xlsx** et **DISPOSITIFS_MED.xlsx** et Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28d8d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sup un encoding: utf-8\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# !pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# !pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# !pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# !pip install spacy transformers sentencepiece torch\n",
    "# !python -m spacy download fr_core_news_md\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install rank-bm25\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "# !pip install openpyxl\n",
    "# !pip install sentencepiece\n",
    "\n",
    "# RAPPEL : relancer le kernel si pb détection de packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb80a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(\"CUDA available :\", torch.cuda.is_available())\n",
    "# print(\"GPU name :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# import torch\n",
    "# print(torch.cuda.memory_allocated() / 1e9, \"GB GPU used\")\n",
    "\n",
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.75it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:04<00:00,  9.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    # device: int = -1  # -1 CPU, 0 GPU\n",
    "    device: int = 0  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \"\"\"\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # calcul similarities restreint\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        sims[i, :] = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy()}\n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # print(df1.head())\n",
    "    # print(df2.head())\n",
    "\n",
    "    # TEMPS : 1min9s\n",
    "    # NLP + traduction (1min9s)\n",
    "    translator = TranslatorFR2EN(device=0)  # GPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "\n",
    "    # TEMPS : 24s\n",
    "    # Matching BM25 -> Embeddings -> Top5 \n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07\n",
    "    )\n",
    "\n",
    "    # # Sauvegarde\n",
    "    # match_wide.to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5.xlsx\", index=False)\n",
    "    # match_wide.attrs[\"out_long\"].to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_long.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98166e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df1 anglais\n",
    "# print(df1p.head()) \n",
    "\n",
    "# # df2 anglais\n",
    "# print(df2p.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab021d65",
   "metadata": {},
   "source": [
    "### 2bis) Version où on remplace la similarité cosinus par une pénalisation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b61439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.62it/s]\n",
      "Batches: 100%|██████████| 43/43 [00:04<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcul TF-IDF...\n",
      "Calcul similarités hybrides (embeddings + TF-IDF)...\n",
      "Calcul métriques d'incertitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8686/3628384705.py:316: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = spearmanr(embedding_sims_all[i], tfidf_sims_all[i])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0) I/O + sélection colonnes df2\n",
    "# ============================================================\n",
    "\n",
    "DF2_KEEP_COLS = [\n",
    "    \"Nomenclature achat\",\n",
    "    \"Catégories d'achat\\n(N-2)\",\n",
    "    \"Segments  d'achat\\n(N-3)\",\n",
    "    \"Sous-segment\",\n",
    "    \"Produit élémentaire\",\n",
    "    \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "]\n",
    "\n",
    "def load_and_select_df2(path_df2_xlsx: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(path_df2_xlsx)\n",
    "    missing = [c for c in DF2_KEEP_COLS if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes dans df2: {missing}\\nColonnes trouvées: {list(df2.columns)}\")\n",
    "    return df2[DF2_KEEP_COLS].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Prétraitement + traduction (identique logique)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "\n",
    "def build_fr_nlp(model_name: str = \"fr_core_news_md\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def build_en_nlp(model_name: str = \"en_core_web_sm\"):\n",
    "    import spacy\n",
    "    return spacy.load(model_name, disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def preprocess_fr(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_fr_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "def preprocess_en(texts: Iterable[str], nlp=None) -> List[str]:\n",
    "    if nlp is None:\n",
    "        nlp = build_en_nlp()\n",
    "    out = []\n",
    "    for doc in nlp.pipe([(\"\" if x is None else str(x)) for x in texts], batch_size=256):\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            if t.is_space or t.is_punct or t.like_num:\n",
    "                continue\n",
    "            if t.is_stop:\n",
    "                continue\n",
    "            lem = (t.lemma_ or t.text).lower().strip()\n",
    "            if len(lem) < 2:\n",
    "                continue\n",
    "            toks.append(lem)\n",
    "        out.append(\" \".join(toks))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class TranslatorFR2EN:\n",
    "    model_name: str = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    # device: int = -1  # -1 CPU, 0 GPU\n",
    "    device: int = 0  # -1 CPU, 0 GPU\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pipe = pipeline(\"translation\", model=self.model_name, device=self.device)\n",
    "\n",
    "    def translate(self, texts: Iterable[str], batch_size: int = 16) -> List[str]:\n",
    "        texts_list = [(\"\" if x is None else str(x)) for x in texts]\n",
    "        outputs = self.pipe(texts_list, batch_size=batch_size, truncation=True)\n",
    "        return [o[\"translation_text\"] for o in outputs]\n",
    "\n",
    "\n",
    "def add_processed_columns(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_produit: str = \"produit\",\n",
    "    col_df2_best: str = \"Produit élémentaire\",\n",
    "    translator: Optional[TranslatorFR2EN] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    On se concentre sur 'Produit élémentaire' pour df2 (champ le plus proche),\n",
    "    et on garde aussi un champ df2 '__df2_join_en_proc' pour éventuellement enrichir.\n",
    "    \"\"\"\n",
    "    if col_df1_produit not in df1.columns:\n",
    "        raise ValueError(f\"df1 n'a pas la colonne {col_df1_produit}\")\n",
    "    if col_df2_best not in df2.columns:\n",
    "        raise ValueError(f\"df2 n'a pas la colonne {col_df2_best}\")\n",
    "\n",
    "    fr_nlp = build_fr_nlp()\n",
    "    en_nlp = build_en_nlp()\n",
    "    if translator is None:\n",
    "        translator = TranslatorFR2EN()\n",
    "\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "\n",
    "    # df1 produit\n",
    "    df1[\"produit_fr_proc\"] = preprocess_fr(df1[col_df1_produit].astype(str), nlp=fr_nlp)\n",
    "    df1[\"produit_en\"] = translator.translate(df1[\"produit_fr_proc\"].tolist())\n",
    "    df1[\"produit_en_proc\"] = preprocess_en(df1[\"produit_en\"], nlp=en_nlp)\n",
    "\n",
    "    # df2 produit élémentaire (principal)\n",
    "    df2[\"produit_elem_fr_proc\"] = preprocess_fr(df2[col_df2_best].astype(str), nlp=fr_nlp)\n",
    "    df2[\"produit_elem_en\"] = translator.translate(df2[\"produit_elem_fr_proc\"].tolist())\n",
    "    df2[\"produit_elem_en_proc\"] = preprocess_en(df2[\"produit_elem_en\"], nlp=en_nlp)\n",
    "\n",
    "    # champ joint optionnel (pondération: Produit élémentaire x3)\n",
    "    # utile si tu veux plus tard intégrer d'autres colonnes, sans casser l'approche\n",
    "    df2[\"__df2_join_en_proc\"] = (\n",
    "        (df2[\"produit_elem_en_proc\"].fillna(\"\") + \" \") * 3\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Filtre lexical BM25 (avant embeddings)\n",
    "# ============================================================\n",
    "\n",
    "def bm25_candidates(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retourne un tableau d'indices (n_df2, topk_bm25) : les meilleurs candidats df1\n",
    "    pour chaque ligne df2 selon BM25.\n",
    "\n",
    "    On tokenize simplement par split() car les textes sont déjà normalisés.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    corpus_tokens = [str(x).split() for x in df1[df1_text_col].fillna(\"\").tolist()]\n",
    "    bm25 = BM25Okapi(corpus_tokens)\n",
    "\n",
    "    cand_idx = np.zeros((df2.shape[0], topk_bm25), dtype=int)\n",
    "\n",
    "    for i, q in enumerate(df2[df2_text_col].fillna(\"\").tolist()):\n",
    "        q_tokens = str(q).split()\n",
    "        scores = bm25.get_scores(q_tokens)  # (n_df1,)\n",
    "        best = np.argsort(-scores)[:topk_bm25]\n",
    "        cand_idx[i, :] = best\n",
    "\n",
    "    return cand_idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Rerank embeddings sur candidats + proba Top-5\n",
    "# ============================================================\n",
    "\n",
    "def embed_texts(texts: List[str], model_name: str = \"pritamdeka/S-PubMedBert-MS-MARCO\") -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    emb = model.encode(texts, normalize_embeddings=True, batch_size=64, show_progress_bar=True)\n",
    "    return np.asarray(emb)\n",
    "\n",
    "def softmax(x: np.ndarray, temperature: float = 0.07) -> np.ndarray:\n",
    "    x = x / max(temperature, 1e-6)\n",
    "    x = x - x.max(axis=1, keepdims=True)\n",
    "    expx = np.exp(x)\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "# ║  DIFFÉRENCE ENTRE LES DEUX APPROCHES DE SIMILARITÉ                            ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# 1) SIMILARITÉ COSINUS PURE (commentée ci-dessous) :\n",
    "#    - Mesure uniquement l'orientation des vecteurs embeddings\n",
    "#    - Ignore complètement les mots exacts utilisés\n",
    "#    - Exemple : \"cathéter veineux\" vs \"tube sanguin\" → score élevé (sémantique)\n",
    "#    - Problème : peut matcher des produits sémantiquement proches mais techniquement \n",
    "#      différents (ex: \"seringue 5ml\" vs \"seringue 10ml\")\n",
    "\n",
    "# 2) SIMILARITÉ HYBRIDE COSINUS + TF-IDF (implémentation actuelle) :\n",
    "#    - Combine sémantique (embeddings) + lexical (TF-IDF)\n",
    "#    - TF-IDF donne plus de poids aux termes rares/spécifiques\n",
    "#    - Exemple : \"cathéter ventriculaire\" → \"ventriculaire\" pèse plus lourd que \n",
    "#      \"cathéter\" (plus commun)\n",
    "#    - Avantage : détecte les correspondances exactes de termes techniques tout en \n",
    "#      gardant la compréhension sémantique\n",
    "#    - Paramètre alpha : contrôle l'équilibre entre les deux mesures\n",
    "#      * alpha=1.0 → 100% embeddings (cosinus pur)\n",
    "#      * alpha=0.0 → 100% TF-IDF (lexical pur)\n",
    "#      * alpha=0.6 → compromis pour nomenclatures médicales\n",
    "\n",
    "# INTERPRÉTABILITÉ :\n",
    "# - Score final = (0.6 × similarité_sémantique) + (0.4 × importance_termes_communs)\n",
    "# - Favorise les produits qui sont à la fois :\n",
    "#   1) Sémantiquement proches (compris par le modèle)\n",
    "#   2) Partageant des termes techniques spécifiques\n",
    "\n",
    "\n",
    "\n",
    "def match_with_bm25_then_embeddings(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    col_df1_key: str = \"produit\",\n",
    "    df1_text_col: str = \"produit_en_proc\",\n",
    "    df2_text_col: str = \"produit_elem_en_proc\",\n",
    "    topk_bm25: int = 20,\n",
    "    topk_final: int = 5,\n",
    "    embedding_model: str = \"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "    temperature: float = 0.07,\n",
    "    alpha: float = 0.6,  # Pondération embeddings vs TF-IDF\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline:\n",
    "    - BM25 filtre les candidats df1 (topk_bm25)\n",
    "    - embeddings + TF-IDF rerank uniquement ces candidats\n",
    "    - softmax sur similarités => pseudo-proba\n",
    "    - renvoie un tableau wide top-5 (et long via attrs)\n",
    "    \n",
    "    Paramètres:\n",
    "        alpha: Poids des embeddings (1-alpha = poids TF-IDF)\n",
    "               alpha=1.0 → similarité cosinus pure\n",
    "               alpha=0.6 → recommandé (équilibre sémantique/lexical)\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from scipy.stats import spearmanr\n",
    "\n",
    "    # df1 unique\n",
    "    df1u = df1[[col_df1_key, df1_text_col]].drop_duplicates(subset=[col_df1_key]).reset_index(drop=True)\n",
    "\n",
    "    # candidats BM25\n",
    "    cand_idx = bm25_candidates(\n",
    "        df1u,\n",
    "        df2,\n",
    "        df1_text_col=df1_text_col,\n",
    "        df2_text_col=df2_text_col,\n",
    "        topk_bm25=topk_bm25,\n",
    "    )  # (n2, topk_bm25)\n",
    "\n",
    "    # embeddings df1 (une seule fois)\n",
    "    emb1 = embed_texts(df1u[df1_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # embeddings df2 (sur champ principal)\n",
    "    emb2 = embed_texts(df2[df2_text_col].fillna(\"\").tolist(), model_name=embedding_model)\n",
    "\n",
    "    # TF-IDF sur corpus df1 unique\n",
    "    print(\"Calcul TF-IDF...\")\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_matrix_df1 = vectorizer.fit_transform(df1u[df1_text_col].fillna(\"\"))\n",
    "    \n",
    "    # TF-IDF sur df2\n",
    "    tfidf_matrix_df2 = vectorizer.transform(df2[df2_text_col].fillna(\"\"))\n",
    "\n",
    "    # calcul similarities restreint (HYBRIDE)\n",
    "    n2 = df2.shape[0]\n",
    "    sims = np.empty((n2, topk_bm25), dtype=float)\n",
    "    \n",
    "    # Stockage pour calcul d'incertitude\n",
    "    embedding_sims_all = np.empty((n2, topk_bm25), dtype=float)\n",
    "    tfidf_sims_all = np.empty((n2, topk_bm25), dtype=float)\n",
    "\n",
    "    print(\"Calcul similarités hybrides (embeddings + TF-IDF)...\")\n",
    "    for i in range(n2):\n",
    "        idx = cand_idx[i]\n",
    "        \n",
    "        # # APPROCHE 1 (commentée) : Similarité cosinus pure sur embeddings\n",
    "        # cos_sim = emb2[i] @ emb1[idx].T  # cosine car normalisé\n",
    "        # sims[i, :] = cos_sim\n",
    "        \n",
    "        # APPROCHE 2 (actuelle) : Hybride embeddings + TF-IDF\n",
    "        # 1) Similarité sémantique (embeddings)\n",
    "        embedding_sim = emb2[i] @ emb1[idx].T  # (topk_bm25,)\n",
    "        \n",
    "        # 2) Similarité lexicale (TF-IDF sur termes communs)\n",
    "        tfidf_sim = (tfidf_matrix_df2[i] @ tfidf_matrix_df1[idx].T).toarray()[0]  # (topk_bm25,)\n",
    "        \n",
    "        # Stockage pour métriques d'incertitude\n",
    "        embedding_sims_all[i, :] = embedding_sim\n",
    "        tfidf_sims_all[i, :] = tfidf_sim\n",
    "        \n",
    "        # 3) Combinaison pondérée\n",
    "        sims[i, :] = alpha * embedding_sim + (1 - alpha) * tfidf_sim\n",
    "\n",
    "    probs = softmax(sims, temperature=temperature)  # (n2, topk_bm25)\n",
    "    \n",
    "    # ============================================================\n",
    "    # QUANTIFICATION D'INCERTITUDE\n",
    "    # ============================================================\n",
    "    \n",
    "    # 1) Entropie normalisée de la distribution de probabilités\n",
    "    # Interprétation: >0.7 = très incertain (probs uniformes)\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
    "    normalized_entropy = entropy / np.log(topk_bm25)\n",
    "    \n",
    "    # 2) Variance des similarités brutes (avant softmax)\n",
    "    # Interprétation: variance élevée = scores bien différenciés (bon signal)\n",
    "    #                 variance faible = tous les candidats se ressemblent (mauvais signal)\n",
    "    sim_variance = np.var(sims, axis=1)\n",
    "    \n",
    "    # 3) Désaccord entre embeddings et TF-IDF (corrélation de Spearman)\n",
    "    # Interprétation: >0.5 = sémantique et lexique pointent vers candidats différents\n",
    "    print(\"Calcul métriques d'incertitude...\")\n",
    "    disagreement = np.zeros(n2)\n",
    "    for i in range(n2):\n",
    "        corr, _ = spearmanr(embedding_sims_all[i], tfidf_sims_all[i])\n",
    "        disagreement[i] = 1 - corr  # 0=accord parfait, 1=désaccord total\n",
    "    \n",
    "    # ============================================================\n",
    "\n",
    "    # topk_final parmi candidats\n",
    "    top_local = np.argsort(-probs, axis=1)[:, :topk_final]             # indices 0..topk_bm25-1\n",
    "    top_prob = np.take_along_axis(probs, top_local, axis=1)            # (n2, topk_final)\n",
    "    top_global_idx = np.take_along_axis(cand_idx, top_local, axis=1)   # indices dans df1u\n",
    "    top_prod = df1u[col_df1_key].to_numpy()[top_global_idx]            # (n2, topk_final)\n",
    "\n",
    "    # outputs\n",
    "    rows = []\n",
    "    for i in range(n2):\n",
    "        for r in range(topk_final):\n",
    "            rows.append({\n",
    "                \"Nomenclature achat\": df2.iloc[i][\"Nomenclature achat\"],\n",
    "                \"rank\": r + 1,\n",
    "                \"produit_match\": top_prod[i, r],\n",
    "                \"proba\": float(top_prob[i, r]),\n",
    "            })\n",
    "    out_long = pd.DataFrame(rows)\n",
    "\n",
    "    wide = {\n",
    "        \"Nomenclature achat\": df2[\"Nomenclature achat\"].to_numpy(),\n",
    "        \"uncertainty_entropy\": normalized_entropy,\n",
    "        \"similarity_variance\": sim_variance,\n",
    "        \"embedding_tfidf_disagreement\": disagreement,\n",
    "    }\n",
    "    \n",
    "    for r in range(topk_final):\n",
    "        wide[f\"top{r+1}_produit\"] = top_prod[:, r]\n",
    "        wide[f\"top{r+1}_proba\"] = top_prob[:, r]\n",
    "    out_wide = pd.DataFrame(wide)\n",
    "\n",
    "    out_wide.attrs[\"out_long\"] = out_long\n",
    "    return out_wide\n",
    "\n",
    "# ============================================================\n",
    "# 4) Utilitaires pratiques\n",
    "# ============================================================\n",
    "\n",
    "def keep_df2_columns(df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mêmes colonnes que tu veux conserver\n",
    "    keep = [\n",
    "        \"Nomenclature achat\",\n",
    "        \"Catégories d'achat\\n(N-2)\",\n",
    "        \"Segments  d'achat\\n(N-3)\",\n",
    "        \"Sous-segment\",\n",
    "        \"Produit élémentaire\",\n",
    "        \"Code des Catégories Homogènes \\nde fournitures et prestations\",\n",
    "    ]\n",
    "    missing = [c for c in keep if c not in df2.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Colonnes manquantes: {missing}\\nColonnes df2: {list(df2.columns)}\")\n",
    "    return df2[keep].copy()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Exemple d'exécution\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # path_df1 = r\"df_composant_medical_emissions_carbones.xlsx\"\n",
    "    # path_df2 = r\"DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    path_df1 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/df_composant_medical_emissions_carbones.xlsx\"\n",
    "    path_df2 = r\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/DISPOSITIFS_MED.xlsx\"\n",
    "\n",
    "    df1 = pd.read_excel(path_df1)\n",
    "    df2 = load_and_select_df2(path_df2)\n",
    "\n",
    "    # print(df1.head())\n",
    "    # print(df2.head())\n",
    "\n",
    "    # TEMPS : 1min8s\n",
    "    # NLP + traduction (1min9s)\n",
    "    translator = TranslatorFR2EN(device=0)  # GPU\n",
    "    df1p, df2p = add_processed_columns(\n",
    "        df1, df2,\n",
    "        col_df1_produit=\"produit\",\n",
    "        col_df2_best=\"Produit élémentaire\",\n",
    "        translator=translator\n",
    "    )\n",
    "\n",
    "\n",
    "    # TEMPS : 9.7s\n",
    "    # Matching BM25 -> Embeddings+TF-IDF -> Top5 \n",
    "    match_wide = match_with_bm25_then_embeddings(\n",
    "        df1p, df2p,\n",
    "        col_df1_key=\"produit\",\n",
    "        df1_text_col=\"produit_en_proc\",\n",
    "        df2_text_col=\"produit_elem_en_proc\",\n",
    "        topk_bm25=20,         \n",
    "        topk_final=5,\n",
    "        embedding_model=\"pritamdeka/S-PubMedBert-MS-MARCO\",\n",
    "        temperature=0.07,\n",
    "        alpha=0.70  # 70% similarité cos, 30% TF-IDF\n",
    "        # alpha=0  # 70% similarité cos, 30% TF-IDF\n",
    "    )\n",
    "\n",
    "    # # Sauvegarde\n",
    "    # match_wide.to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_simcos_et_TF_IDF.xlsx\", index=False)\n",
    "    # match_wide.attrs[\"out_long\"].to_excel(\"/home/onyxia/datachallenge2026/sujets/chu/Axe_2/results/MATCH_df2_vers_df1_top5_long_simcos_et_TF_IDF.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34e87b6",
   "metadata": {},
   "source": [
    "### 3) Réduction et évaluation des coûts carbones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d582a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLM, TintBERT ?\n",
    "# Métriques et graphes de consommation carbonne par inférence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
